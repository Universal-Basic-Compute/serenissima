#!/usr/bin/env python3
"""
Script de diagnostic pour tester le chargement des mod√®les Hugging Face.
Utile pour d√©boguer les probl√®mes de chargement de mod√®les r√©cents.

Usage:
    python diagnose_model.py [--model MODEL_NAME]
"""

import argparse
import transformers
from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM
import torch
import os
import sys

def main():
    parser = argparse.ArgumentParser(description="Diagnostiquer le chargement d'un mod√®le Hugging Face")
    parser.add_argument("--model", type=str, default="deepseek-ai/DeepSeek-R1-0528-Qwen3-8B", 
                        help="Nom ou ID du mod√®le √† tester")
    parser.add_argument("--verbose", action="store_true", help="Afficher des informations d√©taill√©es")
    
    args = parser.parse_args()
    model_name = args.model
    verbose = args.verbose
    
    print(f"üîç Diagnostic du mod√®le: {model_name}")
    print(f"üìö Transformers version: {transformers.__version__}")
    print(f"üî• PyTorch version: {torch.__version__}")
    print(f"üíª CUDA disponible: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"üñ•Ô∏è GPU: {torch.cuda.get_device_name(0)}")
    
    # 1. Tester la configuration
    print("\n1Ô∏è‚É£ Test de la configuration...")
    try:
        config = AutoConfig.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        print(f"‚úÖ Configuration charg√©e")
        print(f"   Architecture: {config.architectures if hasattr(config, 'architectures') else 'Non sp√©cifi√©e'}")
        print(f"   Model type: {config.model_type}")
        if verbose:
            print(f"   Configuration compl√®te: {config}")
    except Exception as e:
        print(f"‚ùå Erreur config: {e}")
    
    # 2. Tester le tokenizer
    print("\n2Ô∏è‚É£ Test du tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        print(f"‚úÖ Tokenizer charg√©")
        print(f"   Taille du vocabulaire: {tokenizer.vocab_size if hasattr(tokenizer, 'vocab_size') else 'Non disponible'}")
        print(f"   Tokens sp√©ciaux: {tokenizer.all_special_tokens}")
        
        # Test de tokenization
        test_text = "Bonjour, je suis un marchand v√©nitien."
        tokens = tokenizer(test_text)
        print(f"   Test de tokenization: '{test_text}' ‚Üí {len(tokens['input_ids'])} tokens")
    except Exception as e:
        print(f"‚ùå Erreur tokenizer: {e}")
    
    # 3. Tester le mod√®le avec diff√©rentes approches
    print("\n3Ô∏è‚É£ Test du mod√®le...")
    approaches = [
        ("AutoModelForCausalLM", AutoModelForCausalLM),
        ("AutoModel", transformers.AutoModel),
    ]
    
    model_loaded = False
    for name, model_class in approaches:
        try:
            print(f"\nTest avec {name}...")
            model = model_class.from_pretrained(
                model_name,
                trust_remote_code=True,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True,
                device_map="auto"
            )
            print(f"‚úÖ {name} fonctionne!")
            
            # Afficher des informations sur le mod√®le
            if hasattr(model, "config"):
                if hasattr(model.config, "hidden_size"):
                    print(f"   Taille cach√©e: {model.config.hidden_size}")
                if hasattr(model.config, "num_hidden_layers"):
                    print(f"   Nombre de couches: {model.config.num_hidden_layers}")
            
            # Test de g√©n√©ration
            if hasattr(model, "generate") and callable(model.generate):
                try:
                    print("\n4Ô∏è‚É£ Test de g√©n√©ration...")
                    inputs = tokenizer("Bonjour, je suis", return_tensors="pt").to(model.device)
                    outputs = model.generate(inputs["input_ids"], max_new_tokens=20, do_sample=True)
                    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    print(f"   Texte g√©n√©r√©: {generated_text}")
                except Exception as e:
                    print(f"‚ùå Erreur lors de la g√©n√©ration: {e}")
            
            model_loaded = True
            break
        except Exception as e:
            print(f"‚ùå {name} erreur: {e}")
    
    if not model_loaded:
        print("\n‚ùå √âchec du chargement du mod√®le avec toutes les approches")
        print("\nüí° Suggestions:")
        print("   1. V√©rifiez que vous avez la derni√®re version de transformers:")
        print("      pip install --upgrade transformers")
        print("   2. Assurez-vous d'avoir suffisamment de m√©moire GPU/RAM")
        print("   3. Assurez-vous d'avoir suffisamment de m√©moire GPU/RAM")
        print("   4. V√©rifiez si le mod√®le n√©cessite des d√©pendances sp√©cifiques")
    else:
        print("\n‚úÖ Diagnostic termin√© avec succ√®s!")

if __name__ == "__main__":
    main()
