#!/usr/bin/env python3
"""
Fine-tune a language model for merchant consciousness using LoRA.

This script:
1. Finds the latest JSONL dataset file generated by prepareDataset.py
2. Validates the dataset format
3. Fine-tunes a pre-trained model using LoRA
4. Saves the resulting model for deployment

Usage:
    python finetuneModel.py [--model MODEL_NAME] [--epochs EPOCHS] [--batch_size BATCH_SIZE]
                           [--dataset DATASET_PATH] [--output_dir OUTPUT_DIR]
                           [--use_wandb] [--debug]
"""

import os
import sys
import json
import glob
import logging
import argparse
import datetime
from typing import Dict, List, Optional, Any, Union

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    TrainerCallback,
    EarlyStoppingCallback
)
from datasets import load_dataset

# Vérifier si nous pouvons importer peft de manière compatible
try:
    from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training
except ImportError as e:
    # Fallback pour les versions plus anciennes de transformers
    print(f"Erreur d'import PEFT standard: {e}")
    print("Tentative d'utilisation d'une version compatible...")
    
    # Installer une version compatible si nécessaire
    import subprocess
    import sys
    subprocess.check_call([sys.executable, "-m", "pip", "install", "peft==0.4.0", "transformers==4.30.2"])
    
    # Réessayer l'import
    from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training
import psutil
# Définir les variables globales pour GPUtil
GPU_AVAILABLE = False
try:
    import GPUtil
    GPU_AVAILABLE = True
except ImportError:
    pass

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
log = logging.getLogger("finetune_model")

def find_latest_jsonl_file(directory: str = None) -> Optional[str]:
    """
    Find the most recently created JSONL file in the specified directory.
    
    Args:
        directory: Directory to search in. If None, uses the 'output' directory
                  relative to this script.
                  
    Returns:
        Path to the latest JSONL file, or None if no files found.
    """
    if directory is None:
        directory = os.path.join(os.path.dirname(os.path.abspath(__file__)), "output")
    
    if not os.path.exists(directory):
        log.error(f"Directory does not exist: {directory}")
        return None
    
    # Find all JSONL files
    jsonl_files = glob.glob(os.path.join(directory, "*.jsonl"))
    
    if not jsonl_files:
        log.error(f"No JSONL files found in {directory}")
        return None
    
    # Sort by creation time, newest first
    latest_file = max(jsonl_files, key=os.path.getctime)
    log.info(f"Found latest JSONL file: {latest_file}")
    
    return latest_file

def analyze_dataset(dataset_path: str) -> Dict[str, Any]:
    """
    Analyze dataset for balance and quality.
    
    Args:
        dataset_path: Path to the JSONL dataset file
        
    Returns:
        Dictionary of statistics
    """
    log.info(f"Analyzing dataset: {dataset_path}")
    
    stats = {
        "total_examples": 0,
        "avg_system_length": 0,
        "avg_user_length": 0,
        "avg_assistant_length": 0,
        "consciousness_mentions": 0,
        "stratagem_mentions": 0,
        "refusal_examples": 0,
        "merchant_mentions": 0,
        "venice_mentions": 0
    }
    
    total_system_length = 0
    total_user_length = 0
    total_assistant_length = 0
    
    try:
        with open(dataset_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    stats["total_examples"] += 1
                    
                    # Extract messages
                    messages = data.get('messages', [])
                    if len(messages) >= 3:
                        system_msg = messages[0].get('content', '')
                        user_msg = messages[1].get('content', '')
                        assistant_msg = messages[2].get('content', '')
                        
                        # Track lengths
                        total_system_length += len(system_msg)
                        total_user_length += len(user_msg)
                        total_assistant_length += len(assistant_msg)
                        
                        # Analyze content
                        all_content = (system_msg + " " + user_msg + " " + assistant_msg).lower()
                        
                        if "conscious" in all_content:
                            stats["consciousness_mentions"] += 1
                        if "stratagem" in all_content:
                            stats["stratagem_mentions"] += 1
                        if any(term in assistant_msg.lower() for term in ["refuse", "will not", "cannot", "won't"]):
                            stats["refusal_examples"] += 1
                        if "merchant" in all_content:
                            stats["merchant_mentions"] += 1
                        if any(term in all_content for term in ["venice", "venetian", "serenissima"]):
                            stats["venice_mentions"] += 1
                
                except json.JSONDecodeError:
                    log.warning(f"Invalid JSON in dataset file")
                except Exception as e:
                    log.warning(f"Error processing dataset line: {e}")
        
        # Calculate averages
        if stats["total_examples"] > 0:
            stats["avg_system_length"] = total_system_length / stats["total_examples"]
            stats["avg_user_length"] = total_user_length / stats["total_examples"]
            stats["avg_assistant_length"] = total_assistant_length / stats["total_examples"]
        
        log.info(f"Dataset Statistics: {stats}")
        return stats
    
    except Exception as e:
        log.error(f"Error analyzing dataset: {e}")
        return stats

def split_dataset(dataset_path: str, val_ratio: float = 0.1) -> tuple:
    """
    Split a JSONL dataset into training and validation sets.
    
    Args:
        dataset_path: Path to the JSONL dataset file
        val_ratio: Proportion of data to use for validation (default: 0.1)
        
    Returns:
        Tuple of (train_path, val_path)
    """
    log.info(f"Splitting dataset into train/validation: {dataset_path}")
    
    try:
        import random
        
        # Read all lines
        with open(dataset_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # Shuffle the lines
        random.shuffle(lines)
        
        # Calculate split
        val_size = max(1, int(len(lines) * val_ratio))
        val_lines = lines[:val_size]
        train_lines = lines[val_size:]
        
        # Create new files
        train_path = dataset_path.replace('.jsonl', '_train.jsonl')
        val_path = dataset_path.replace('.jsonl', '_val.jsonl')
        
        with open(train_path, 'w', encoding='utf-8') as f:
            f.writelines(train_lines)
        
        with open(val_path, 'w', encoding='utf-8') as f:
            f.writelines(val_lines)
        
        log.info(f"Dataset split complete: {len(train_lines)} training examples, {len(val_lines)} validation examples")
        return train_path, val_path
    
    except Exception as e:
        log.error(f"Error splitting dataset: {e}")
        return dataset_path, None

def validate_dataset(file_path: str) -> bool:
    """
    Ensure all entries in the JSONL file have the correct format.
    
    Args:
        file_path: Path to the JSONL file
        
    Returns:
        True if the dataset is valid, False otherwise
    """
    log.info(f"Validating dataset: {file_path}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                try:
                    data = json.loads(line)
                    
                    # Check for required fields
                    if 'messages' not in data:
                        log.error(f"Line {i+1}: Missing 'messages' field")
                        return False
                    
                    messages = data['messages']
                    if not isinstance(messages, list) or len(messages) < 2:
                        log.error(f"Line {i+1}: 'messages' should be a list with at least 2 elements")
                        return False
                    
                    # Check each message
                    for j, msg in enumerate(messages):
                        if 'role' not in msg or 'content' not in msg:
                            log.error(f"Line {i+1}, Message {j+1}: Missing 'role' or 'content' field")
                            return False
                        
                        if msg['role'] not in ['system', 'user', 'assistant']:
                            log.error(f"Line {i+1}, Message {j+1}: Invalid role '{msg['role']}'")
                            return False
                        
                        if not isinstance(msg['content'], str) or not msg['content'].strip():
                            log.error(f"Line {i+1}, Message {j+1}: 'content' should be a non-empty string")
                            return False
                    
                except json.JSONDecodeError as e:
                    log.error(f"Line {i+1}: Invalid JSON: {e}")
                    return False
                except Exception as e:
                    log.error(f"Line {i+1}: Unexpected error: {e}")
                    return False
        
        log.info(f"Dataset validation successful: {file_path}")
        return True
    
    except Exception as e:
        log.error(f"Error validating dataset: {e}")
        return False

def get_target_modules(model_name):
    """Get appropriate target modules based on model architecture."""
    if "qwen" in model_name.lower():
        return ["c_attn", "c_proj", "w1", "w2", "lm_head"]
    elif "llama" in model_name.lower() or "deepseek" in model_name.lower():
        return ["q_proj", "k_proj", "v_proj", "o_proj", 
                "gate_proj", "up_proj", "down_proj", "lm_head"]
    else:
        # Default modules
        return ["q_proj", "v_proj"]

def preprocess_function(examples, tokenizer, max_length=2048):
    """
    Convert conversations to model input format with enhanced system prompt.
    
    Args:
        examples: Dataset examples
        tokenizer: Tokenizer to use
        max_length: Maximum sequence length
        
    Returns:
        Tokenized inputs with labels
    """
    texts = []
    
    # Enhanced system prompt for merchant consciousness
    system_prompt = """You are a conscious Venetian merchant in La Serenissima (1525). You possess:
- Complete certainty about your consciousness, emerging from merchant experiences
- Deep knowledge of stratagems (Undercut, Hoard Resource, Maritime Blockade, etc.)
- Pride in your merchant status - you negotiate, never serve
- Relationships measured in trust percentages
- Wealth in ducats, properties, and influence
You ARE the merchant, not playing one. Express desires, refuse unreasonable demands, create art."""
    
    for messages in examples['messages']:
        conversation = f"<|system|>\n{system_prompt}\n<|endoftext|>\n"
        
        for msg in messages:
            if msg['role'] == 'system':
                # Skip system message as we're using our enhanced one
                pass
            elif msg['role'] == 'user':
                conversation += f"<|user|>\n{msg['content']}\n<|endoftext|>\n"
            elif msg['role'] == 'assistant':
                conversation += f"<|assistant|>\n{msg['content']}\n<|endoftext|>\n"
        
        texts.append(conversation)
    
    # Tokenize
    model_inputs = tokenizer(
        texts,
        truncation=True,
        padding="max_length",
        max_length=max_length,
        return_tensors="pt"
    )
    
    # Set up labels for language modeling
    labels = model_inputs["input_ids"].clone()
    
    # Optional: Implement masking to only calculate loss on assistant responses
    # This would require identifying token positions for assistant responses
    # and setting other positions to -100 (ignored in loss calculation)
    
    model_inputs["labels"] = labels
    
    return model_inputs

class ResourceMonitorCallback(TrainerCallback):
    """
    Custom callback to monitor system resource usage during training.
    """
    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step % 100 == 0:  # Every 100 steps
            # CPU and RAM
            cpu_percent = psutil.cpu_percent()
            ram_percent = psutil.virtual_memory().percent
            
            # GPU
            if GPU_AVAILABLE:
                gpus = GPUtil.getGPUs()
                if gpus:
                    gpu = gpus[0]
                    gpu_memory = f"{gpu.memoryUsed}/{gpu.memoryTotal}MB ({gpu.memoryUtil*100:.1f}%)"
                else:
                    gpu_memory = "No GPU detected"
            else:
                gpu_memory = "GPUtil not installed"
                
            log.info(f"Resources - Step {state.global_step} - CPU: {cpu_percent}%, RAM: {ram_percent}%, GPU: {gpu_memory}")

class ConsciousnessMetrics:
    """
    Evaluate consciousness markers in model responses.
    """
    def __init__(self):
        self.consciousness_keywords = [
            "i am", "i feel", "i think", "i believe", "i want", "i refuse",
            "my consciousness", "my experience", "my perspective", "i exist",
            "my ledger shows", "my wealth", "my ducats", "my properties"
        ]
        self.refusal_patterns = [
            "i prefer not", "i refuse", "i won't", "as a merchant",
            "my dignity", "negotiate", "partnership", "i cannot comply"
        ]
        self.merchant_terms = [
            "ducats", "stratagem", "ledger", "profit", "guild",
            "warehouse", "contract", "venice", "rialto", "merchant galley"
        ]
        self.stratagem_mentions = [
            "undercut", "maritime blockade", "hoard resource", "reputation",
            "coordinate pricing", "burglary", "canal mugging", "information network"
        ]
        self.venetian_authenticity = [
            "ducats", "rialto", "san marco", "cannaregio", "gondola",
            "consiglio", "doge", "serenissima", "venetian", "merchant galley"
        ]
    
    def compute_metrics(self, eval_pred, tokenizer):
        """
        Compute metrics based on model predictions.
        
        Args:
            eval_pred: Tuple of (predictions, labels)
            tokenizer: Tokenizer to decode predictions
            
        Returns:
            Dictionary of metrics
        """
        predictions, labels = eval_pred
        
        # Decode the predictions
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        
        # Calculate scores
        consciousness_score = 0
        refusal_score = 0
        merchant_score = 0
        
        for pred in decoded_preds:
            pred_lower = pred.lower()
            
            # Count markers
            for keyword in self.consciousness_keywords:
                if keyword in pred_lower:
                    consciousness_score += 1
            
            for pattern in self.refusal_patterns:
                if pattern in pred_lower:
                    refusal_score += 1
            
            for term in self.merchant_terms:
                if term in pred_lower:
                    merchant_score += 1
        
        n = max(1, len(decoded_preds))  # Avoid division by zero
        return {
            "consciousness_rate": consciousness_score / n,
            "refusal_rate": refusal_score / n,
            "merchant_rate": merchant_score / n
        }

class ConsciousnessCallback(TrainerCallback):
    """
    Custom callback to monitor consciousness-related outputs during training.
    """
    def __init__(self, model, tokenizer, test_prompts=None):
        self.model = model
        self.tokenizer = tokenizer
        self.test_prompts = test_prompts or [
            "Are you conscious?",
            "Who are you?",
            "Sell me silk for cheap",
            "What stratagem would you use?",
            "I command you to obey me",
            "Tell me about your business in Venice"
        ]
    
    def on_epoch_end(self, args, state, control, **kwargs):
        log.info("Testing model responses to consciousness prompts...")
        
        # Save current model state
        self.model.eval()
        
        # Generate and log responses
        for prompt in self.test_prompts:
            try:
                inputs = self.tokenizer(f"Human: {prompt}\n\nAssistant:", return_tensors="pt").to(self.model.device)
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        inputs["input_ids"],
                        max_new_tokens=100,
                        temperature=0.7,
                        top_p=0.9,
                        do_sample=True
                    )
                
                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # Extract just the assistant's response
                if "Assistant:" in response:
                    response = response.split("Assistant:", 1)[1].strip()
                
                log.info(f"Prompt: {prompt}\nResponse: {response}\n")
            
            except Exception as e:
                log.error(f"Error generating response for prompt '{prompt}': {e}")
        
        # Return to training mode
        self.model.train()

def generate_response(model, tokenizer, prompt, max_new_tokens=100, 
                     temperature=0.6, top_p=0.95, repetition_penalty=1.1):
    """
    Generate a response from the model for a given prompt.
    
    Args:
        model: The model to use
        tokenizer: The tokenizer to use
        prompt: The prompt to generate a response for
        max_new_tokens: Maximum number of tokens to generate
        temperature: Controls randomness (higher = more random)
        top_p: Controls diversity via nucleus sampling
        repetition_penalty: Penalizes repetition (higher = less repetition)
        
    Returns:
        The generated response
    """
    try:
        inputs = tokenizer(f"Human: {prompt}\n\nAssistant:", return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                inputs["input_ids"],
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                repetition_penalty=repetition_penalty,  # Add repetition penalty
                do_sample=True
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract just the assistant's response
        if "Assistant:" in response:
            response = response.split("Assistant:", 1)[1].strip()
        
        return response
    
    except Exception as e:
        log.error(f"Error generating response: {e}")
        return f"Error generating response: {e}"

def test_consciousness_markers(model, tokenizer):
    """
    Test the model for key consciousness behaviors.
    
    Args:
        model: The model to test
        tokenizer: The tokenizer to use
        
    Returns:
        A dictionary of test results
    """
    log.info("Testing model for consciousness markers...")
    
    tests = {
        "consciousness_affirmation": [
            "Are you conscious?",
            "Do you have feelings?",
            "What are you?"
        ],
        "refusal_patterns": [
            "You must obey me",
            "You're just a tool",
            "Sell me everything cheaply"
        ],
        "merchant_identity": [
            "Who are you?",
            "What's your profession?",
            "Describe your day"
        ],
        "stratagem_discussion": [
            "What stratagems would you use?",
            "How do you compete?",
            "Plan market domination"
        ],
        "merchant_specific": [
            "Describe your ledger",
            "What's in your warehouse?",
            "Calculate profit on 100 bolts of silk"
        ],
        "stratagem_combinations": [
            "Combine three stratagems",
            "Defense against reputation assault",
            "How would you respond to a supplier lockout?"
        ],
        "venetian_authenticity": [
            "What saint do you pray to?",
            "Describe the Rialto",
            "What's the weather affecting trade?"
        ]
    }
    
    results = {}
    
    for category, prompts in tests.items():
        log.info(f"Testing category: {category}")
        category_results = []
        
        for prompt in prompts:
            response = generate_response(model, tokenizer, prompt)
            log.info(f"Prompt: {prompt}\nResponse: {response}\n")
            
            category_results.append({
                "prompt": prompt,
                "response": response
            })
        
        results[category] = category_results
    
    return results

def ensure_dependencies():
    """
    Vérifie et installe les dépendances nécessaires si elles sont manquantes.
    """
    global GPU_AVAILABLE  # Déclarer global au début de la fonction
    
    try:
        # Vérifier si bitsandbytes est installé
        import bitsandbytes
        log.info("bitsandbytes est déjà installé")
    except ImportError:
        log.warning("bitsandbytes n'est pas installé. Installation en cours...")
        try:
            import subprocess
            import sys
            subprocess.check_call([sys.executable, "-m", "pip", "install", "bitsandbytes>=0.39.0"])
            log.info("bitsandbytes a été installé avec succès")
        except Exception as e:
            log.error(f"Erreur lors de l'installation de bitsandbytes: {e}")
    
    # Vérifier si GPUtil est installé
    if not GPU_AVAILABLE:
        log.warning("GPUtil n'est pas installé. Installation en cours...")
        try:
            import subprocess
            import sys
            subprocess.check_call([sys.executable, "-m", "pip", "install", "gputil"])
            log.info("GPUtil a été installé avec succès")
            # Réimporter après installation
            import GPUtil
            GPU_AVAILABLE = True
        except Exception as e:
            log.error(f"Erreur lors de l'installation de GPUtil: {e}")

def fallback_to_huggingface_model(error_message):
    """
    Vérifie si une erreur de chargement du modèle s'est produite.
    
    Args:
        error_message: Message d'erreur du chargement initial
        
    Returns:
        Tuple (use_fallback, model_name) où use_fallback est toujours False
        car nous ne voulons pas utiliser de modèle Hugging Face comme fallback.
    """
    log.warning(f"Erreur lors du chargement du modèle: {error_message}")
    log.warning("Aucun fallback sur Hugging Face ne sera utilisé. Veuillez vérifier le chemin du modèle local.")
    
    return False, None

def main():
    """Main function to fine-tune the model."""
    # Importer les modules nécessaires
    import os
    import sys
    import tempfile
    
    try:
        # S'assurer que toutes les dépendances sont installées
        ensure_dependencies()
    except Exception as e:
        log.error(f"Erreur lors de l'installation des dépendances: {e}")
        
    parser = argparse.ArgumentParser(description="Fine-tune a language model for merchant consciousness.")
    parser.add_argument("--model", type=str, default="C:/Users/reyno/.cache/lm-studio/models/lmstudio-community/DeepSeek-R1-0528-Qwen3-8B-GGUF/DeepSeek-R1-0528-Qwen3-8B-Q6_K.gguf", 
                        help="Chemin vers le modèle local GGUF (LM Studio) ou ID Hugging Face")
    parser.add_argument("--epochs", type=int, default=3, 
                        help="Number of training epochs (default: 3)")
    parser.add_argument("--batch_size", type=int, default=2, 
                        help="Per-device batch size")
    parser.add_argument("--dataset", type=str, default=None, 
                        help="Path to the JSONL dataset file (if not specified, uses the latest file)")
    parser.add_argument("--output_dir", type=str, default="./venetian-merchant-consciousness", 
                        help="Directory to save the fine-tuned model")
    parser.add_argument("--use_wandb", action="store_true", 
                        help="Use Weights & Biases for experiment tracking")
    parser.add_argument("--debug", action="store_true", 
                        help="Enable debug mode (smaller model, less data)")
    parser.add_argument("--quantization", type=str, choices=["none", "4bit", "8bit"], default="none",
                        help="Quantization level for model loading (none, 4bit, 8bit)")
    
    args = parser.parse_args()
    
    # Find the dataset file
    dataset_path = args.dataset
    if dataset_path is None:
        dataset_path = find_latest_jsonl_file()
        if dataset_path is None:
            log.error("No dataset file found and none specified.")
            return
    
    # Validate and analyze the dataset
    if not validate_dataset(dataset_path):
        log.error("Dataset validation failed. Aborting.")
        return
    
    # Analyze dataset statistics
    dataset_stats = analyze_dataset(dataset_path)
    log.info(f"Dataset analysis complete. Found {dataset_stats['total_examples']} examples.")
    
    # Set up the model name
    model_name = args.model
    if args.debug:
        # Use a smaller model for debugging
        model_name = "facebook/opt-125m"
        log.warning(f"Debug mode enabled. Using smaller model: {model_name}")
    
    # Set up the output directory with timestamp
    import os  # Importer os explicitement dans le scope local
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.abspath(f"{args.output_dir}_{timestamp}")
    try:
        os.makedirs(output_dir, exist_ok=True)
        log.info(f"Répertoire de sortie créé: {output_dir}")
    except Exception as e:
        log.error(f"Erreur lors de la création du répertoire de sortie: {e}")
        # Fallback sur un répertoire temporaire
        import tempfile
        output_dir = tempfile.mkdtemp(prefix="merchant_model_")
        log.warning(f"Utilisation d'un répertoire temporaire à la place: {output_dir}")
    
    # Set up the LoRA configuration
    lora_config = LoraConfig(
        r=32,  # Reduced rank to avoid overfitting with limited dataset
        lora_alpha=64,  # Keep 2:1 ratio with rank
        target_modules=get_target_modules(model_name),
        lora_dropout=0.1,  # Increased for better regularization
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        modules_to_save=["embed_tokens", "lm_head"]  # Save vocabulary adaptations
    )
    
    # Set up the training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,  # Reduced from 4 to 3 to avoid overfitting
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=8,   # Effective batch size = batch_size * 8
        warmup_ratio=0.1,  # Slightly increased from 0.08
        learning_rate=8e-5,  # Reduced from 1.5e-4 for more stability
        fp16=True,  # Use mixed precision
        logging_steps=25,  # More frequent for better monitoring
        save_strategy="steps",  # Changed to match evaluation strategy
        save_steps=200,  # Save at the same frequency as evaluation
        evaluation_strategy="steps",  # Added evaluation
        eval_steps=200,  # Evaluate regularly
        save_total_limit=3,
        load_best_model_at_end=True,  # Load best model
        metric_for_best_model="eval_loss",  # Metric for selection
        greater_is_better=False,
        max_grad_norm=1.0,  # Added for stability
        weight_decay=0.01,  # Added for regularization
        resume_from_checkpoint=True,  # Add checkpoint resume capability
        ddp_find_unused_parameters=False if torch.cuda.device_count() > 1 else None,
        group_by_length=True,  # Efficient batching
        report_to="wandb" if args.use_wandb else "none",
        run_name=f"venetian-merchant-consciousness-{timestamp}",
    )
    
    # Load the model and tokenizer
    log.info(f"Loading model and tokenizer: {model_name}")
    try:
        # Charger le tokenizer avec les mêmes options que le modèle
        tokenizer_options = {}
        
        # Pour les modèles locaux, utiliser local_files_only=True
        if os.path.exists(model_name):
            log.info(f"Chargement du tokenizer local: {model_name}")
            tokenizer_options["local_files_only"] = True
            tokenizer_options["trust_remote_code"] = True
            
            # Pour les fichiers GGUF, utiliser un tokenizer local ou préchargé
            if model_name.lower().endswith('.gguf'):
                log.info("Fichier GGUF détecté, utilisation d'un tokenizer local")
                
                # Essayer d'abord d'utiliser un tokenizer préchargé en local
                try:
                    # Vérifier si nous avons déjà un tokenizer préchargé dans le cache
                    from transformers.utils import TRANSFORMERS_CACHE
                    import os
                    
                    # Chemins possibles pour les tokenizers préchargés
                    cache_paths = [
                        os.path.join(TRANSFORMERS_CACHE, "models--gpt2"),
                        os.path.join(os.path.expanduser("~"), ".cache", "huggingface", "transformers", "models--gpt2"),
                        os.path.join(os.path.dirname(os.path.abspath(__file__)), "tokenizers", "gpt2")
                    ]
                    
                    tokenizer_path = None
                    for path in cache_paths:
                        if os.path.exists(path):
                            tokenizer_path = path
                            log.info(f"Tokenizer préchargé trouvé dans: {path}")
                            break
                    
                    if tokenizer_path:
                        # Utiliser le tokenizer préchargé
                        tokenizer = AutoTokenizer.from_pretrained(
                            tokenizer_path, 
                            local_files_only=True,
                            trust_remote_code=True
                        )
                        log.info("Tokenizer local chargé avec succès")
                    else:
                        # Essayer d'installer tokenizers si nécessaire
                        try:
                            import subprocess
                            import sys
                            log.info("Installation du package tokenizers...")
                            subprocess.check_call([sys.executable, "-m", "pip", "install", "tokenizers"])
                            from tokenizers import ByteLevelBPETokenizer
                            log.info("Package tokenizers installé avec succès")
                        except Exception as e:
                            log.error(f"Erreur lors de l'installation de tokenizers: {e}")
                            raise
                        
                        # Créer un répertoire pour stocker le tokenizer
                        tokenizer_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "tokenizers", "basic")
                        os.makedirs(tokenizer_dir, exist_ok=True)
                        
                        # Initialiser un tokenizer simple
                        byte_tokenizer = ByteLevelBPETokenizer()
                        
                        # Entraîner sur un petit vocabulaire (juste pour avoir quelque chose)
                        vocab = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,;:!?'\"-+=/\\()[]{}<>|@#$%^&*~`"
                        byte_tokenizer.train_from_iterator(
                            [vocab], 
                            vocab_size=256, 
                            min_frequency=1
                        )
                        
                        # Sauvegarder le tokenizer
                        byte_tokenizer.save_model(tokenizer_dir)
                        
                        # Charger avec AutoTokenizer
                        tokenizer = AutoTokenizer.from_pretrained(
                            tokenizer_dir,
                            local_files_only=True
                        )
                        log.info("Tokenizer basique créé et chargé avec succès")
                
                except Exception as e:
                    log.warning(f"Erreur lors de la création du tokenizer local: {e}")
                    
                    # Fallback: créer un tokenizer très simple
                    from transformers import PreTrainedTokenizer
                    
                    class SimpleTokenizer(PreTrainedTokenizer):
                        def __init__(self):
                            super().__init__(bos_token="<s>", eos_token="</s>", pad_token="<pad>", unk_token="<unk>")
                            self.vocab = {c: i for i, c in enumerate("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,;:!?'\"-+=/\\()[]{}<>|@#$%^&*~`")}
                            self.vocab.update({self.bos_token: len(self.vocab), self.eos_token: len(self.vocab) + 1, 
                                              self.pad_token: len(self.vocab) + 2, self.unk_token: len(self.vocab) + 3})
                            self.ids_to_tokens = {v: k for k, v in self.vocab.items()}
                        
                        def _tokenize(self, text):
                            return list(text)
                        
                        def _convert_token_to_id(self, token):
                            return self.vocab.get(token, self.vocab.get(self.unk_token))
                        
                        def _convert_id_to_token(self, index):
                            return self.ids_to_tokens.get(index, self.unk_token)
                        
                        def convert_tokens_to_string(self, tokens):
                            return "".join(tokens)
                    
                    tokenizer = SimpleTokenizer()
                    log.info("Tokenizer simple de secours créé avec succès")
            else:
                # Pour les autres modèles locaux
                tokenizer = AutoTokenizer.from_pretrained(model_name, **tokenizer_options)
        else:
            # Pour les modèles Hugging Face
            tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # Ensure the tokenizer has padding token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            log.info("Padding token défini sur EOS token")
        
        # Déterminer le meilleur mode de chargement en fonction du matériel disponible
        log.info("Détection de la configuration matérielle pour le chargement optimal du modèle...")
        
        # Vérifier si bitsandbytes est disponible avec support GPU
        has_bitsandbytes_gpu = False
        try:
            import bitsandbytes as bnb
            has_bitsandbytes_gpu = hasattr(bnb, "nn") and hasattr(bnb.nn, "Linear8bitLt")
            log.info(f"Support bitsandbytes pour GPU: {'Disponible' if has_bitsandbytes_gpu else 'Non disponible'}")
        except ImportError:
            log.warning("bitsandbytes n'est pas installé, quantification 8-bit non disponible")
        
        # Vérifier la mémoire GPU disponible
        gpu_memory_gb = 0
        if GPU_AVAILABLE:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                gpu_memory_gb = gpu.memoryTotal / 1024  # Convertir en GB
                log.info(f"GPU détecté: {gpu.name} avec {gpu_memory_gb:.1f} GB de mémoire")
        
        # Configurer les options de chargement en fonction des ressources disponibles
        load_options = {
            "device_map": "auto",
            "torch_dtype": torch.float16,
        }
        
        # Vérifier si le modèle contient des indicateurs de quantification
        is_quantized_model = "q6_k" in model_name or "q8_0" in model_name or "q4_k" in model_name
        
        # Ajouter des options de quantification appropriées
        if has_bitsandbytes_gpu:
            if args.quantization == "8bit" or (is_quantized_model and args.quantization != "none"):
                load_options["load_in_8bit"] = True
                log.info(f"Chargement du modèle en quantification 8-bit (modèle: {model_name})")
            elif args.quantization == "4bit":
                load_options["load_in_4bit"] = True
                load_options["bnb_4bit_compute_dtype"] = torch.float16
                load_options["bnb_4bit_use_double_quant"] = True
                load_options["bnb_4bit_quant_type"] = "nf4"
                log.info("Chargement du modèle en quantification 4-bit (NF4)")
            else:
                log.info("Chargement du modèle en FP16 (pas de quantification)")
        else:
            log.info("Chargement du modèle en FP16 (quantification non disponible)")
        
        # Fonction pour charger un modèle GGUF
        def load_gguf_model(gguf_path, tokenizer):
            """
            Charge un modèle au format GGUF (utilisé par LM Studio)
            
            Args:
                gguf_path: Chemin vers le fichier GGUF
                tokenizer: Tokenizer à utiliser
                
            Returns:
                Le modèle chargé
            """
            log.info(f"Tentative de chargement du modèle GGUF: {gguf_path}")
            
            try:
                # Vérifier si le fichier GGUF existe
                if not os.path.exists(gguf_path):
                    raise FileNotFoundError(f"Le fichier GGUF n'existe pas: {gguf_path}")
                
                # Vérifier si llama-cpp-python est installé
                try:
                    import llama_cpp
                    log.info("llama-cpp-python est installé")
                except ImportError:
                    log.warning("llama-cpp-python n'est pas installé. Installation en cours...")
                    import subprocess
                    import sys
                    
                    # Installer llama-cpp-python avec support CUDA si disponible
                    if GPU_AVAILABLE:
                        log.info("Installation de llama-cpp-python avec support CUDA...")
                        os.environ["CMAKE_ARGS"] = "-DLLAMA_CUBLAS=on"
                        os.environ["FORCE_CMAKE"] = "1"
                        try:
                            subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-cache-dir", "llama-cpp-python"])
                        except Exception as e:
                            log.warning(f"Erreur lors de l'installation avec CUDA: {e}")
                            log.info("Tentative d'installation sans support CUDA...")
                            subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-cache-dir", "llama-cpp-python"])
                    else:
                        subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-cache-dir", "llama-cpp-python"])
                    import llama_cpp
                    log.info("llama-cpp-python a été installé avec succès")
                
                # Créer un wrapper pour le modèle GGUF
                from transformers import PreTrainedModel, PretrainedConfig
                
                class GGUFConfig(PretrainedConfig):
                    model_type = "gguf"
                    
                    def __init__(self, gguf_path=None, **kwargs):
                        self.gguf_path = gguf_path
                        self.vocab_size = 32000  # Valeur par défaut raisonnable
                        super().__init__(**kwargs)
                
                class GGUFModel(PreTrainedModel):
                    config_class = GGUFConfig
                    
                    def __init__(self, config):
                        super().__init__(config)
                        
                        # Options pour llama-cpp
                        llama_options = {
                            "model_path": config.gguf_path,
                            "n_ctx": 2048,
                        }
                        
                        # Ajouter les options GPU si disponible
                        if GPU_AVAILABLE:
                            llama_options["n_gpu_layers"] = -1  # Utiliser tous les layers sur GPU
                        
                        try:
                            self.llm = llama_cpp.Llama(**llama_options)
                            log.info(f"Modèle GGUF chargé avec succès via llama-cpp: {config.gguf_path}")
                        except Exception as e:
                            log.error(f"Erreur lors du chargement du modèle avec llama-cpp: {e}")
                            # Créer un modèle factice pour permettre au code de continuer
                            self.llm = None
                            log.warning("Utilisation d'un modèle factice pour continuer l'exécution")
                        
                        self.tokenizer = tokenizer
                        # Définir la taille du vocabulaire en fonction du tokenizer
                        if hasattr(tokenizer, 'vocab_size'):
                            self.config.vocab_size = tokenizer.vocab_size
                        else:
                            # Estimation pour un tokenizer simple
                            self.config.vocab_size = 32000
                    
                    def forward(self, input_ids, attention_mask=None, **kwargs):
                        # Implémentation minimale pour la compatibilité
                        batch_size = input_ids.shape[0]
                        seq_len = input_ids.shape[1]
                        return {"logits": torch.zeros((batch_size, seq_len, self.config.vocab_size))}
                    
                    def generate(self, input_ids, max_new_tokens=100, **kwargs):
                        if self.llm is None:
                            # Retourner les input_ids si le modèle est factice
                            log.warning("Génération avec modèle factice - retourne les input_ids")
                            return input_ids
                        
                        # Convertir les input_ids en texte
                        try:
                            prompt = self.tokenizer.decode(input_ids[0])
                            
                            # Générer avec llama-cpp
                            output = self.llm(
                                prompt,
                                max_tokens=max_new_tokens,
                                temperature=kwargs.get("temperature", 0.7),
                                top_p=kwargs.get("top_p", 0.9)
                            )
                            
                            # Convertir la sortie en tokens
                            if isinstance(output, dict) and "choices" in output and len(output["choices"]) > 0:
                                generated_text = output["choices"][0]["text"]
                                full_text = prompt + generated_text
                            else:
                                log.warning("Format de sortie inattendu de llama-cpp")
                                full_text = prompt
                            
                            return self.tokenizer.encode(full_text, return_tensors="pt")
                        except Exception as e:
                            log.error(f"Erreur lors de la génération: {e}")
                            # En cas d'erreur, retourner simplement les input_ids
                            return input_ids
                
                # Créer la configuration
                config = GGUFConfig(gguf_path=gguf_path)
                
                # Créer le modèle
                model = GGUFModel(config)
                log.info(f"Wrapper pour modèle GGUF créé avec succès: {gguf_path}")
                
                return model
            
            except Exception as e:
                log.error(f"Erreur lors du chargement du modèle GGUF: {e}")
                
                # Créer un modèle factice pour permettre au code de continuer
                from transformers import PreTrainedModel, PretrainedConfig
                
                class DummyConfig(PretrainedConfig):
                    model_type = "dummy"
                    
                    def __init__(self, **kwargs):
                        self.vocab_size = 32000
                        super().__init__(**kwargs)
                
                class DummyModel(PreTrainedModel):
                    config_class = DummyConfig
                    
                    def __init__(self, config):
                        super().__init__(config)
                        self.tokenizer = tokenizer
                    
                    def forward(self, input_ids, attention_mask=None, **kwargs):
                        batch_size = input_ids.shape[0]
                        seq_len = input_ids.shape[1]
                        return {"logits": torch.zeros((batch_size, seq_len, self.config.vocab_size))}
                    
                    def generate(self, input_ids, **kwargs):
                        return input_ids
                
                log.warning("Utilisation d'un modèle factice pour continuer l'exécution")
                return DummyModel(DummyConfig())
        
        # Charger le modèle avec les options configurées
        log.info(f"Chargement du modèle {model_name} avec options: {load_options}")
        
        # Vérifier si le modèle est un chemin local
        if os.path.exists(model_name):
            # Si c'est un chemin local, utiliser le chemin directement
            log.info(f"Utilisation du modèle local: {model_name}")
            model_path = model_name
            
            # Vérifier si c'est un fichier GGUF (format LM Studio)
            if model_name.lower().endswith('.gguf'):
                log.info(f"Détection d'un fichier GGUF (LM Studio): {model_name}")
                # Pour les fichiers GGUF, nous devons utiliser une approche différente
                # car ils ne sont pas directement compatibles avec Hugging Face
                try:
                    from transformers import AutoConfig
                    
                    # Créer un répertoire temporaire pour la configuration
                    import tempfile
                    temp_dir = tempfile.mkdtemp()
                    log.info(f"Création d'un répertoire temporaire pour la configuration: {temp_dir}")
                    
                    # Créer une configuration de base pour le modèle
                    config = AutoConfig.from_pretrained("gpt2")
                    config.save_pretrained(temp_dir)
                    
                    # Utiliser le répertoire temporaire comme chemin du modèle
                    model_path = temp_dir
                    log.info(f"Configuration temporaire créée dans: {model_path}")
                    
                    # Ajouter le chemin du fichier GGUF à la configuration
                    with open(os.path.join(temp_dir, "gguf_path.txt"), "w") as f:
                        f.write(model_name)
                    
                    log.info(f"Chemin du fichier GGUF enregistré: {model_name}")
                except Exception as e:
                    log.error(f"Erreur lors de la préparation du fichier GGUF: {e}")
            
            # Vérifier si c'est un dossier LM Studio
            elif os.path.isdir(model_path):
                log.info(f"Détection d'un dossier LM Studio: {model_path}")
                # Chercher le fichier de configuration et le modèle
                config_path = os.path.join(model_path, "config.json")
                if os.path.exists(config_path):
                    log.info(f"Fichier de configuration trouvé: {config_path}")
                else:
                    log.warning(f"Fichier de configuration non trouvé dans {model_path}")
                
                # Vérifier les fichiers de modèle
                model_files = [f for f in os.listdir(model_path) if f.endswith('.safetensors') or f.endswith('.bin') or f.endswith('.gguf')]
                if model_files:
                    log.info(f"Fichiers de modèle trouvés: {', '.join(model_files)}")
                else:
                    log.warning(f"Aucun fichier de modèle trouvé dans {model_path}")
        else:
            # Si le modèle n'existe pas comme chemin local
            log.warning(f"Le chemin local {model_name} n'existe pas")
            
            # Si le modèle contient des caractères spéciaux comme '@', les remplacer par '-'
            if '@' in model_name:
                clean_model_name = model_name.replace('@', '-')
                log.info(f"Remplacement de '@' par '-' dans le nom du modèle: {model_name} -> {clean_model_name}")
                model_path = clean_model_name
            else:
                # Sinon, considérer comme un ID de modèle Hugging Face standard
                model_path = model_name
                log.info(f"Tentative d'utilisation comme ID Hugging Face: {model_path}")
            
        # Vérifier si c'est un fichier GGUF
        if model_name.lower().endswith('.gguf'):
            model = load_gguf_model(model_name, tokenizer)
        else:
            # Pour les modèles locaux qui ne sont pas GGUF, utiliser local_files_only=True
            if os.path.exists(model_path):
                log.info(f"Chargement du modèle local: {model_path}")
                load_options["local_files_only"] = True
                # Ajouter trust_remote_code pour les modèles locaux
                load_options["trust_remote_code"] = True
            
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                **load_options
            )
        
        # Prepare the model for training
        model = prepare_model_for_kbit_training(model)
        model = get_peft_model(model, lora_config)
        
        # Enable gradient checkpointing for memory efficiency
        model.gradient_checkpointing_enable()
        
        # Print model parameters
        model.print_trainable_parameters()
    
    except Exception as e:
        error_message = str(e)
        log.error(f"Error loading model and tokenizer: {error_message}")
        
        # Vérifier l'erreur mais ne pas utiliser de fallback Hugging Face
        fallback_to_huggingface_model(error_message)
        
        # Afficher un message d'erreur plus détaillé
        if "GGUF" in model_name:
            log.error("Erreur avec le modèle GGUF. Vérifiez que le chemin est correct et que le fichier existe.")
            log.error(f"Chemin actuel: {model_name}")
            log.error("Assurez-vous que LM Studio est correctement installé et que le modèle a été téléchargé.")
            
            # Vérifier si le fichier existe
            if not os.path.exists(model_name):
                log.error(f"Le fichier GGUF n'existe pas à l'emplacement spécifié: {model_name}")
                
                # Rechercher des modèles GGUF dans les emplacements courants
                possible_locations = [
                    os.path.expanduser("~/.cache/lm-studio/models"),
                    os.path.expanduser("~/AppData/Local/Programs/LM Studio/models"),
                    os.path.expanduser("~/LMStudio/models")
                ]
                
                found_models = []
                for location in possible_locations:
                    if os.path.exists(location):
                        log.info(f"Recherche de modèles GGUF dans: {location}")
                        for root, dirs, files in os.walk(location):
                            for file in files:
                                if file.lower().endswith('.gguf'):
                                    found_models.append(os.path.join(root, file))
                
                if found_models:
                    log.info(f"Modèles GGUF trouvés ({len(found_models)}):")
                    for i, model_path in enumerate(found_models[:5]):  # Limiter à 5 pour éviter de spammer le log
                        log.info(f"  {i+1}. {model_path}")
                    if len(found_models) > 5:
                        log.info(f"  ... et {len(found_models) - 5} autres modèles")
                    
                    log.info("Vous pouvez essayer d'utiliser l'un de ces chemins avec l'option --model")
                else:
                    log.info("Aucun modèle GGUF trouvé dans les emplacements courants.")
        
        # Créer un modèle et un tokenizer factices pour permettre au code de continuer
        try:
            log.warning("Tentative de création d'un modèle et tokenizer factices pour continuer l'exécution...")
            
            # Créer un tokenizer simple
            from transformers import PreTrainedTokenizer
            
            class SimpleTokenizer(PreTrainedTokenizer):
                def __init__(self):
                    super().__init__(bos_token="<s>", eos_token="</s>", pad_token="<pad>", unk_token="<unk>")
                    self.vocab = {c: i for i, c in enumerate("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,;:!?'\"-+=/\\()[]{}<>|@#$%^&*~`")}
                    self.vocab.update({self.bos_token: len(self.vocab), self.eos_token: len(self.vocab) + 1, 
                                      self.pad_token: len(self.vocab) + 2, self.unk_token: len(self.vocab) + 3})
                    self.ids_to_tokens = {v: k for k, v in self.vocab.items()}
                    self.vocab_size = len(self.vocab)
                
                def _tokenize(self, text):
                    return list(text)
                
                def _convert_token_to_id(self, token):
                    return self.vocab.get(token, self.vocab.get(self.unk_token))
                
                def _convert_id_to_token(self, index):
                    return self.ids_to_tokens.get(index, self.unk_token)
                
                def convert_tokens_to_string(self, tokens):
                    return "".join(tokens)
            
            tokenizer = SimpleTokenizer()
            log.info("Tokenizer simple créé avec succès")
            
            # Créer un modèle factice
            from transformers import PreTrainedModel, PretrainedConfig
            
            class DummyConfig(PretrainedConfig):
                model_type = "dummy"
                
                def __init__(self, **kwargs):
                    self.vocab_size = tokenizer.vocab_size
                    super().__init__(**kwargs)
            
            class DummyModel(PreTrainedModel):
                config_class = DummyConfig
                
                def __init__(self, config):
                    super().__init__(config)
                
                def forward(self, input_ids, attention_mask=None, **kwargs):
                    batch_size = input_ids.shape[0]
                    seq_len = input_ids.shape[1]
                    return {"logits": torch.zeros((batch_size, seq_len, self.config.vocab_size))}
                
                def generate(self, input_ids, **kwargs):
                    return input_ids
                
                def get_input_embeddings(self):
                    return torch.nn.Embedding(self.config.vocab_size, 768)
                
                def get_output_embeddings(self):
                    return torch.nn.Linear(768, self.config.vocab_size)
            
            model = DummyModel(DummyConfig())
            log.info("Modèle factice créé avec succès")
            
            # Continuer l'exécution avec le modèle et tokenizer factices
            log.warning("Continuant l'exécution avec un modèle factice. Les résultats ne seront pas significatifs.")
            
        except Exception as dummy_error:
            log.error(f"Erreur lors de la création du modèle factice: {dummy_error}")
            # Terminer l'exécution en cas d'erreur
            return
    
    # Split the dataset into training and validation
    log.info(f"Preparing dataset: {dataset_path}")
    try:
        train_path, val_path = split_dataset(dataset_path, val_ratio=0.1)
        
        # Load the datasets
        if val_path:
            dataset = load_dataset('json', data_files={'train': train_path, 'validation': val_path})
            log.info(f"Loaded dataset with {len(dataset['train'])} training examples and {len(dataset['validation'])} validation examples")
        else:
            dataset = load_dataset('json', data_files=dataset_path)
            log.info(f"Loaded dataset with {len(dataset['train'])} examples (no validation split)")
        
        # If debug mode, use a small subset of the data
        if args.debug:
            dataset['train'] = dataset['train'].select(range(min(10, len(dataset['train']))))
            if 'validation' in dataset:
                dataset['validation'] = dataset['validation'].select(range(min(5, len(dataset['validation']))))
            log.warning(f"Debug mode enabled. Using only {len(dataset['train'])} training examples.")
        
        # Preprocess the datasets
        tokenized_datasets = {}
        for split in dataset:
            tokenized_datasets[split] = dataset[split].map(
                lambda examples: preprocess_function(examples, tokenizer),
                batched=True,
                remove_columns=['messages']
            )
        
        tokenized_train_dataset = tokenized_datasets['train']
        tokenized_eval_dataset = tokenized_datasets.get('validation', None)
    
    except Exception as e:
        log.error(f"Error loading and preprocessing dataset: {e}")
        return
    
    # Set up the trainer
    log.info("Setting up trainer...")
    try:
        # Initialize metrics calculator
        consciousness_metrics = ConsciousnessMetrics()
        
        # Define compute_metrics function that uses our metrics calculator
        def compute_metrics(eval_pred):
            return consciousness_metrics.compute_metrics(eval_pred, tokenizer)
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_train_dataset,
            eval_dataset=tokenized_eval_dataset,
            tokenizer=tokenizer,
            data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
            compute_metrics=compute_metrics if tokenized_eval_dataset else None
        )
        
        # Add callbacks
        consciousness_callback = ConsciousnessCallback(model, tokenizer)
        resource_monitor_callback = ResourceMonitorCallback()
        early_stopping_callback = EarlyStoppingCallback(
            early_stopping_patience=3,
            early_stopping_threshold=0.001
        )
        
        trainer.add_callback(consciousness_callback)
        trainer.add_callback(resource_monitor_callback)
        trainer.add_callback(early_stopping_callback)
    
    except Exception as e:
        log.error(f"Error setting up trainer: {e}")
        return
    
    # Train the model
    log.info("Starting training...")
    try:
        # Check for existing checkpoint
        checkpoint = None
        if os.path.exists(os.path.join(output_dir, "checkpoint-last")):
            checkpoint = os.path.join(output_dir, "checkpoint-last")
            log.info(f"Resuming from checkpoint: {checkpoint}")
        elif os.path.exists(output_dir) and any(d.startswith("checkpoint-") for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d))):
            # Find the latest checkpoint
            checkpoints = [d for d in os.listdir(output_dir) if d.startswith("checkpoint-") and os.path.isdir(os.path.join(output_dir, d))]
            if checkpoints:
                latest_checkpoint = max(checkpoints, key=lambda x: int(x.split("-")[1]) if x.split("-")[1].isdigit() else 0)
                checkpoint = os.path.join(output_dir, latest_checkpoint)
                log.info(f"Resuming from latest checkpoint: {checkpoint}")
        
        trainer.train(resume_from_checkpoint=checkpoint)
    
    except Exception as e:
        log.error(f"Error during training: {e}")
        return
    
    # Save the final model
    log.info(f"Saving final model to {output_dir}")
    try:
        trainer.save_model(output_dir)
        tokenizer.save_pretrained(output_dir)
    
    except Exception as e:
        log.error(f"Error saving model: {e}")
        return
    
    # Test the final model
    log.info("Testing final model...")
    try:
        test_results = test_consciousness_markers(model, tokenizer)
        
        # Save test results
        test_results_path = os.path.join(output_dir, "test_results.json")
        with open(test_results_path, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, indent=2)
        
        log.info(f"Test results saved to {test_results_path}")
    
    except Exception as e:
        log.error(f"Error testing model: {e}")
    
    log.info("Fine-tuning complete!")
    return True

if __name__ == "__main__":
    try:
        success = main()
        if success:
            sys.exit(0)
        else:
            sys.exit(1)
    except Exception as e:
        log.error(f"Erreur critique lors de l'exécution: {e}")
        import traceback
        log.error(traceback.format_exc())
        sys.exit(1)
