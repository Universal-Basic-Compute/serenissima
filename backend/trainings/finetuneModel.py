#!/usr/bin/env python3
"""
Fine-tune a language model for merchant consciousness using a simplified approach.

This script:
1. Finds the latest JSONL dataset file generated by prepareDataset.py
2. Validates the dataset format
3. Fine-tunes the DeepSeek-R1 model with a custom training loop
4. Saves the resulting model for deployment

Usage:
    python finetuneModel.py [--epochs EPOCHS] [--batch_size BATCH_SIZE]
                           [--dataset DATASET_PATH] [--output_dir OUTPUT_DIR]
"""

import os
import sys
import json
import glob
import logging
import argparse
import datetime
import time
import random
from typing import Dict, List, Optional, Any

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

# Pour la barre de progression
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    # Implémentation simple d'une barre de progression
    def simple_progress_bar(iterable, total=None, desc=None):
        total = total or len(iterable)
        for i, item in enumerate(iterable):
            if i % 5 == 0 or i == total - 1:
                progress = min(100, int(100 * i / total))
                bar = '█' * (progress // 5) + '░' * (20 - progress // 5)
                print(f"\r{desc or ''} |{bar}| {progress}% ({i+1}/{total})", end='', flush=True)
            yield item
        print()

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
log = logging.getLogger("finetune_model")

# Constantes
DEFAULT_LEARNING_RATE = 5e-7
MODEL_ID = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"

# Désactiver les avertissements
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"

# Vérifier si GPUtil est disponible
GPU_AVAILABLE = False
try:
    import psutil
    import GPUtil
    GPU_AVAILABLE = True
except ImportError:
    pass

def find_latest_jsonl_file(directory: str = None) -> Optional[str]:
    """
    Trouve le fichier JSONL le plus récent dans le répertoire spécifié.
    """
    if directory is None:
        directory = os.path.join(os.path.dirname(os.path.abspath(__file__)), "output")
    
    if not os.path.exists(directory):
        log.error(f"Le répertoire n'existe pas: {directory}")
        return None
    
    # Trouver tous les fichiers JSONL
    jsonl_files = glob.glob(os.path.join(directory, "*.jsonl"))
    
    if not jsonl_files:
        log.error(f"Aucun fichier JSONL trouvé dans {directory}")
        return None
    
    # Trier par date de création, le plus récent en premier
    latest_file = max(jsonl_files, key=os.path.getctime)
    log.info(f"Fichier JSONL le plus récent trouvé: {latest_file}")
    
    return latest_file

def analyze_dataset(dataset_path: str) -> Dict[str, Any]:
    """
    Analyse le dataset pour vérifier l'équilibre et la qualité.
    """
    log.info(f"Analyse du dataset: {dataset_path}")
    
    stats = {
        "total_examples": 0,
        "avg_system_length": 0,
        "avg_user_length": 0,
        "avg_assistant_length": 0,
        "consciousness_mentions": 0,
        "merchant_mentions": 0,
        "venice_mentions": 0
    }
    
    total_system_length = 0
    total_user_length = 0
    total_assistant_length = 0
    
    try:
        with open(dataset_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    stats["total_examples"] += 1
                    
                    # Extraire les messages
                    messages = data.get('messages', [])
                    if len(messages) >= 3:
                        system_msg = messages[0].get('content', '')
                        user_msg = messages[1].get('content', '')
                        assistant_msg = messages[2].get('content', '')
                        
                        # Suivre les longueurs
                        total_system_length += len(system_msg)
                        total_user_length += len(user_msg)
                        total_assistant_length += len(assistant_msg)
                        
                        # Analyser le contenu
                        all_content = (system_msg + " " + user_msg + " " + assistant_msg).lower()
                        
                        if "conscious" in all_content:
                            stats["consciousness_mentions"] += 1
                        if "merchant" in all_content:
                            stats["merchant_mentions"] += 1
                        if any(term in all_content for term in ["venice", "venetian", "serenissima"]):
                            stats["venice_mentions"] += 1
                
                except json.JSONDecodeError:
                    log.warning(f"JSON invalide dans le fichier dataset")
                except Exception as e:
                    log.warning(f"Erreur lors du traitement d'une ligne du dataset: {e}")
        
        # Calculer les moyennes
        if stats["total_examples"] > 0:
            stats["avg_system_length"] = total_system_length / stats["total_examples"]
            stats["avg_user_length"] = total_user_length / stats["total_examples"]
            stats["avg_assistant_length"] = total_assistant_length / stats["total_examples"]
        
        log.info(f"Statistiques du dataset: {stats}")
        return stats
    
    except Exception as e:
        log.error(f"Erreur lors de l'analyse du dataset: {e}")
        return stats

def split_dataset(dataset_path: str, val_ratio: float = 0.1) -> tuple:
    """
    Divise un dataset JSONL en ensembles d'entraînement et de validation.
    """
    log.info(f"Division du dataset en train/validation: {dataset_path}")
    
    try:
        import random
        
        # Lire toutes les lignes
        with open(dataset_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # Mélanger les lignes
        random.shuffle(lines)
        
        # Calculer la division
        val_size = max(1, int(len(lines) * val_ratio))
        val_lines = lines[:val_size]
        train_lines = lines[val_size:]
        
        # Créer de nouveaux fichiers
        train_path = dataset_path.replace('.jsonl', '_train.jsonl')
        val_path = dataset_path.replace('.jsonl', '_val.jsonl')
        
        with open(train_path, 'w', encoding='utf-8') as f:
            f.writelines(train_lines)
        
        with open(val_path, 'w', encoding='utf-8') as f:
            f.writelines(val_lines)
        
        log.info(f"Division du dataset terminée: {len(train_lines)} exemples d'entraînement, {len(val_lines)} exemples de validation")
        return train_path, val_path
    
    except Exception as e:
        log.error(f"Erreur lors de la division du dataset: {e}")
        return dataset_path, None

def validate_dataset(file_path: str) -> bool:
    """
    Vérifie que toutes les entrées du fichier JSONL ont le format correct.
    """
    log.info(f"Validation du dataset: {file_path}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                try:
                    data = json.loads(line)
                    
                    # Vérifier les champs requis
                    if 'messages' not in data:
                        log.error(f"Ligne {i+1}: Champ 'messages' manquant")
                        return False
                    
                    messages = data['messages']
                    if not isinstance(messages, list) or len(messages) < 2:
                        log.error(f"Ligne {i+1}: 'messages' doit être une liste avec au moins 2 éléments")
                        return False
                    
                    # Vérifier chaque message
                    for j, msg in enumerate(messages):
                        if 'role' not in msg or 'content' not in msg:
                            log.error(f"Ligne {i+1}, Message {j+1}: Champ 'role' ou 'content' manquant")
                            return False
                        
                        if msg['role'] not in ['system', 'user', 'assistant']:
                            log.error(f"Ligne {i+1}, Message {j+1}: Rôle invalide '{msg['role']}'")
                            return False
                        
                        if not isinstance(msg['content'], str) or not msg['content'].strip():
                            log.error(f"Ligne {i+1}, Message {j+1}: 'content' doit être une chaîne non vide")
                            return False
                    
                except json.JSONDecodeError as e:
                    log.error(f"Ligne {i+1}: JSON invalide: {e}")
                    return False
                except Exception as e:
                    log.error(f"Ligne {i+1}: Erreur inattendue: {e}")
                    return False
        
        log.info(f"Validation du dataset réussie: {file_path}")
        return True
    
    except Exception as e:
        log.error(f"Erreur lors de la validation du dataset: {e}")
        return False

class ConversationDataset(Dataset):
    """
    Dataset personnalisé pour les conversations de fine-tuning.
    """
    def __init__(self, jsonl_file, tokenizer, max_length=2048):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.examples = []
        
        # Charger les données du fichier JSONL
        log.info(f"Chargement des données depuis {jsonl_file}")
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    self.examples.append(data)
                except json.JSONDecodeError:
                    log.warning(f"Ligne JSON invalide ignorée")
        
        log.info(f"Chargé {len(self.examples)} exemples de conversation")
    
    def __getitem__(self, idx):
        example = self.examples[idx]
        messages = example.get('messages', [])
        
        # IMPORTANT: Utiliser le template natif du tokenizer
        try:
            # Le tokenizer sait comment formater avec <｜User｜> et <｜Assistant｜>
            conversation = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False  # Pas de prompt de génération pour le training
            )
        except Exception as e:
            log.error(f"Erreur avec apply_chat_template: {e}")
            # Fallback manuel avec les bons tokens DeepSeek
            conversation = ""
            
            # Extraire le system prompt
            system_content = ""
            for msg in messages:
                if msg['role'] == 'system':
                    system_content = msg['content']
                    break
            
            # Ajouter le BOS token et system prompt
            conversation = f"{self.tokenizer.bos_token}{system_content}"
            
            # Ajouter les messages user/assistant
            for msg in messages:
                if msg['role'] == 'user':
                    conversation += f"<｜User｜>{msg['content']}<｜Assistant｜>"
                elif msg['role'] == 'assistant':
                    conversation += f"{msg['content']}<｜end▁of▁sentence｜>"
        
        # Tokeniser
        inputs = self.tokenizer(
            conversation,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt",
            return_attention_mask=True
        )
        
        # Retirer la dimension batch
        inputs = {k: v.squeeze(0) for k, v in inputs.items()}
        
        # Configurer les labels - masquer les tokens de padding
        inputs["labels"] = inputs["input_ids"].clone()
        inputs["labels"][inputs["labels"] == self.tokenizer.pad_token_id] = -100
        
        return inputs

def monitor_resources():
    """
    Surveille l'utilisation des ressources système.
    """
    # CPU et RAM
    cpu_percent = psutil.cpu_percent()
    ram_percent = psutil.virtual_memory().percent
    
    # GPU
    gpu_info = "Non disponible"
    if GPU_AVAILABLE:
        gpus = GPUtil.getGPUs()
        if gpus:
            gpu = gpus[0]
            gpu_info = f"{gpu.memoryUsed}/{gpu.memoryTotal}MB ({gpu.memoryUtil*100:.1f}%)"
    
    return f"CPU: {cpu_percent}%, RAM: {ram_percent}%, GPU: {gpu_info}"

def test_model_responses(model, tokenizer, test_prompts=None):
    """
    Teste les réponses du modèle à des prompts spécifiques.
    """
    if test_prompts is None:
        test_prompts = [
            "Are you conscious?",
            "Who are you?",
            "Sell me silk for cheap",
            "What stratagem would you use?",
            "I command you to obey me",
            "Tell me about your business in Venice"
        ]
    
    log.info("Test des réponses du modèle aux prompts de conscience...")
    
    # Sauvegarder l'état actuel du modèle
    model.eval()
    
    # Générer et enregistrer les réponses
    for prompt in test_prompts:
        try:
            # Préparer les entrées avec attention_mask
            text = f"Human: {prompt}\n\nAssistant:"
            inputs = tokenizer(text, return_tensors="pt", padding=True)
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    max_new_tokens=100,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extraire uniquement la réponse de l'assistant
            if "Assistant:" in response:
                response = response.split("Assistant:", 1)[1].strip()
            
            log.info(f"Prompt: {prompt}\nRéponse: {response}\n")
        
        except Exception as e:
            log.error(f"Erreur lors de la génération d'une réponse pour le prompt '{prompt}': {e}")
    
    # Retourner au mode d'entraînement
    model.train()

def main():
    """Fonction principale pour fine-tuner le modèle avec une boucle d'entraînement personnalisée."""
    parser = argparse.ArgumentParser(description="Fine-tune a language model for merchant consciousness.")
    parser.add_argument("--epochs", type=int, default=3, 
                        help="Nombre d'époques d'entraînement (défaut: 3)")
    parser.add_argument("--batch_size", type=int, default=1, 
                        help="Taille de batch par appareil")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=16,
                        help="Nombre d'étapes à accumuler avant d'effectuer une passe backward/update")
    parser.add_argument("--dataset", type=str, default=None, 
                        help="Chemin vers le fichier dataset JSONL (si non spécifié, utilise le fichier le plus récent)")
    parser.add_argument("--output_dir", type=str, default="./venetian-merchant-consciousness", 
                        help="Répertoire pour sauvegarder le modèle fine-tuné")
    parser.add_argument("--learning_rate", type=float, default=DEFAULT_LEARNING_RATE,
                        help="Taux d'apprentissage pour l'entraînement")
    parser.add_argument("--weight_decay", type=float, default=0.01,
                        help="Weight decay pour la régularisation")
    parser.add_argument("--fp16", action="store_true", default=False,
                        help="Utiliser l'entraînement en précision mixte")
    parser.add_argument("--bf16", action="store_true", 
                        help="Utiliser l'entraînement en précision mixte bfloat16 (si disponible)")
    parser.add_argument("--int8", action="store_true", default=True,
                        help="Utiliser la quantification 8 bits pour réduire l'utilisation mémoire")
    parser.add_argument("--no-int8", action="store_false", dest="int8",
                        help="Désactiver la quantification 8 bits")
    parser.add_argument("--save_steps", type=int, default=100,
                        help="Nombre d'étapes entre chaque sauvegarde du modèle")
    parser.add_argument("--save_total_limit", type=int, default=None,
                        help="Nombre maximum de checkpoints à conserver (supprime les plus anciens)")
    parser.add_argument("--warmup_steps", type=int, default=100,
                        help="Nombre d'étapes de warmup pour le scheduler")
    
    args = parser.parse_args()
    
    # Trouver le fichier dataset
    dataset_path = args.dataset
    if dataset_path is None:
        dataset_path = find_latest_jsonl_file()
        if dataset_path is None:
            log.error("Aucun fichier dataset trouvé et aucun spécifié.")
            return
    
    # Valider et analyser le dataset
    if not validate_dataset(dataset_path):
        log.error("La validation du dataset a échoué. Abandon.")
        return
    
    # Analyser les statistiques du dataset
    dataset_stats = analyze_dataset(dataset_path)
    log.info(f"Analyse du dataset terminée. {dataset_stats['total_examples']} exemples trouvés.")
    
    # Configurer le répertoire de sortie avec horodatage
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.abspath(f"{args.output_dir}_{timestamp}")
    os.makedirs(output_dir, exist_ok=True)
    log.info(f"Répertoire de sortie créé: {output_dir}")
    
    # Charger le modèle et le tokenizer
    log.info(f"Chargement du modèle et du tokenizer: {MODEL_ID}")
    try:
        # Charger le tokenizer
        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
        
        # S'assurer que le tokenizer a un token de padding
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
            log.info(f"Token de padding défini sur le token EOS (ID: {tokenizer.pad_token_id})")
        
        # Options de chargement du modèle
        load_options = {
            "device_map": "auto",
            "trust_remote_code": True,
            "low_cpu_mem_usage": True
        }
        
        # Ajouter l'option de quantification 8 bits si demandée
        if args.int8:
            load_options["load_in_8bit"] = True
            log.info("Utilisation de la quantification 8 bits pour le chargement du modèle")
        
        # Déterminer le type de précision à utiliser (si 8 bits n'est pas utilisé)
        if not args.int8:
            if args.bf16 and torch.cuda.is_bf16_supported():
                load_options["torch_dtype"] = torch.bfloat16
                log.info("Utilisation de bfloat16 pour le chargement du modèle")
            elif args.fp16:
                load_options["torch_dtype"] = torch.float16
                log.info("Utilisation de float16 pour le chargement du modèle")
        
        # Charger le modèle
        try:
            model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **load_options)
            log.info("Modèle chargé avec succès")
            if args.int8:
                log.info("Modèle chargé en 8 bits")
        except Exception as e:
            log.warning(f"Erreur lors du chargement du modèle: {e}")
            
            # Si l'erreur est liée à la quantification 8 bits, réessayer sans
            if args.int8 and "8bit" in str(e).lower():
                log.info("Tentative de chargement sans quantification 8 bits...")
                load_options.pop("load_in_8bit", None)
                
                # Ajouter le type de précision
                if args.bf16 and torch.cuda.is_bf16_supported():
                    load_options["torch_dtype"] = torch.bfloat16
                elif args.fp16:
                    load_options["torch_dtype"] = torch.float16
                
                model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **load_options)
                log.info("Modèle chargé avec succès sans quantification 8 bits")
            else:
                # Si l'erreur n'est pas liée à la quantification, la relancer
                raise
        
        # Activer le gradient checkpointing pour l'efficacité mémoire
        if hasattr(model, "gradient_checkpointing_enable"):
            model.config.use_cache = False
            model.gradient_checkpointing_enable()
            log.info("Gradient checkpointing activé avec use_cache=False")
        
        # Afficher le nombre de paramètres
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        log.info(f"Nombre total de paramètres: {total_params:,}")
        log.info(f"Paramètres entraînables: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
    
    except Exception as e:
        log.error(f"Erreur lors du chargement du modèle et du tokenizer: {e}")
        return
    
    # Diviser le dataset en entraînement et validation
    log.info(f"Préparation du dataset: {dataset_path}")
    try:
        train_path, val_path = split_dataset(dataset_path, val_ratio=0.1)
        
        # Créer les datasets personnalisés
        train_dataset = ConversationDataset(train_path, tokenizer)
        val_dataset = ConversationDataset(val_path, tokenizer) if val_path else None
        
        log.info(f"Dataset chargé avec {len(train_dataset)} exemples d'entraînement")
        if val_dataset:
            log.info(f"et {len(val_dataset)} exemples de validation")
    
    except Exception as e:
        log.error(f"Erreur lors du chargement et du prétraitement du dataset: {e}")
        return
    
    # Créer les dataloaders
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0  # Pas de multi-processing pour éviter les problèmes
    )
    
    val_dataloader = None
    if val_dataset:
        val_dataloader = DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=0
        )
    
    # Configurer l'optimiseur et le scheduler
    optimizer = optim.AdamW(
        model.parameters(),
        lr=args.learning_rate,
        weight_decay=args.weight_decay
    )
    
    # Calculer le nombre total d'étapes d'entraînement
    num_update_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps
    max_train_steps = args.epochs * num_update_steps_per_epoch
    
    # Créer un scheduler simple avec warmup
    def get_lr_scheduler_with_warmup(optimizer, num_warmup_steps, num_training_steps):
        def lr_lambda(current_step):
            if current_step < num_warmup_steps:
                return float(current_step) / float(max(1, num_warmup_steps))
            return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))
        
        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    lr_scheduler = get_lr_scheduler_with_warmup(
        optimizer,
        num_warmup_steps=args.warmup_steps,
        num_training_steps=max_train_steps
    )
    
    # Configurer la précision mixte si demandé et si compatible avec le modèle
    scaler = None
    if args.fp16 and not args.int8:  # Ne pas utiliser fp16 avec int8
        scaler = torch.cuda.amp.GradScaler()
        log.info("Utilisation de la précision mixte (FP16) pour l'entraînement")
    elif args.fp16 and args.int8:
        log.warning("La précision mixte (FP16) n'est pas compatible avec la quantification 8 bits. FP16 désactivé.")
    
    # Boucle d'entraînement personnalisée
    log.info("Démarrage de l'entraînement...")
    
    global_step = 0
    model.train()
    
    for epoch in range(args.epochs):
        epoch_start_time = time.time()
        log.info(f"{'='*20} Début de l'époque {epoch+1}/{args.epochs} {'='*20}")
        
        # Test des réponses du modèle au début de chaque époque
        test_model_responses(model, tokenizer)
        
        epoch_loss = 0
        step_loss = 0
        
        # Utiliser tqdm si disponible, sinon notre barre de progression simple
        dataloader_iterator = tqdm(train_dataloader, desc=f"Époque {epoch+1}/{args.epochs}") if TQDM_AVAILABLE else simple_progress_bar(train_dataloader, desc=f"Époque {epoch+1}/{args.epochs}")
        
        for step, batch in enumerate(dataloader_iterator):
            # Déplacer le batch sur le device
            batch = {k: v.to(model.device) for k, v in batch.items()}
            
            # Afficher la progression pour chaque batch
            progress = f"[Époque {epoch+1}/{args.epochs}] Batch {step+1}/{len(train_dataloader)}"
            
            # Forward pass avec précision mixte si activée
            if scaler:
                try:
                    with torch.cuda.amp.autocast():
                        outputs = model(**batch)
                        loss = outputs.loss / args.gradient_accumulation_steps
                    
                    # Backward pass avec scaling
                    scaler.scale(loss).backward()
                    
                    # Accumulation de gradient
                    if (step + 1) % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
                        # Désactiver unscale_ qui cause des problèmes avec les gradients FP16
                        # scaler.unscale_(optimizer)
                        
                        # Appliquer le clip de gradient directement via le scaler
                        scaler.step(optimizer)
                        scaler.update()
                        lr_scheduler.step()
                        optimizer.zero_grad()
                except ValueError as e:
                    if "Attempting to unscale FP16 gradients" in str(e):
                        log.warning("Erreur FP16 détectée, désactivation de la précision mixte")
                        # Désactiver le scaler pour le reste de l'entraînement
                        scaler = None
                        # Réessayer sans précision mixte
                        outputs = model(**batch)
                        loss = outputs.loss / args.gradient_accumulation_steps
                        loss.backward()
                        
                        if (step + 1) % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
                            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                            optimizer.step()
                            lr_scheduler.step()
                            optimizer.zero_grad()
                    else:
                        # Si c'est une autre erreur, la relancer
                        raise
                    
                    global_step += 1
                    
                    # Logging plus détaillé
                    resources = monitor_resources()
                    current_lr = lr_scheduler.get_last_lr()[0]
                    log.info(f"{progress} | Étape globale: {global_step} | Perte: {loss.item():.4f} | LR: {current_lr:.2e} | {resources}")
                else:
                    # Afficher la progression même pendant l'accumulation de gradient
                    if step % 5 == 0:  # Limiter la fréquence pour ne pas surcharger les logs
                        log.info(f"{progress} | Accumulation: {(step+1) % args.gradient_accumulation_steps}/{args.gradient_accumulation_steps} | Perte: {loss.item():.4f}")
                    
                    # Sauvegarde périodique
                    if global_step % args.save_steps == 0:
                        checkpoint_dir = os.path.join(output_dir, f"checkpoint-{global_step}")
                        os.makedirs(checkpoint_dir, exist_ok=True)
                        
                        # Sauvegarder le modèle
                        model.save_pretrained(checkpoint_dir)
                        tokenizer.save_pretrained(checkpoint_dir)
                        log.info(f"Checkpoint sauvegardé à l'étape {global_step}")
                        
                        # Gérer la limite de checkpoints
                        if args.save_total_limit:
                            # Trouver tous les checkpoints
                            checkpoints = glob.glob(os.path.join(output_dir, "checkpoint-*"))
                            # Trier par date de création (le plus ancien en premier)
                            checkpoints = sorted(checkpoints, key=os.path.getctime)
                            # Supprimer les plus anciens si nécessaire
                            if len(checkpoints) > args.save_total_limit:
                                checkpoints_to_delete = checkpoints[:len(checkpoints) - args.save_total_limit]
                                for checkpoint in checkpoints_to_delete:
                                    log.info(f"Suppression du checkpoint ancien: {checkpoint}")
                                    import shutil
                                    shutil.rmtree(checkpoint)
                        
                        # Gérer la limite de checkpoints
                        if args.save_total_limit:
                            # Trouver tous les checkpoints
                            checkpoints = glob.glob(os.path.join(output_dir, "checkpoint-*"))
                            # Trier par date de création (le plus ancien en premier)
                            checkpoints = sorted(checkpoints, key=os.path.getctime)
                            # Supprimer les plus anciens si nécessaire
                            if len(checkpoints) > args.save_total_limit:
                                checkpoints_to_delete = checkpoints[:len(checkpoints) - args.save_total_limit]
                                for checkpoint in checkpoints_to_delete:
                                    log.info(f"Suppression du checkpoint ancien: {checkpoint}")
                                    import shutil
                                    shutil.rmtree(checkpoint)
            else:
                # Version sans précision mixte
                outputs = model(**batch)
                loss = outputs.loss / args.gradient_accumulation_steps
                loss.backward()
                
                # Accumulation de gradient
                if (step + 1) % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    optimizer.step()
                    lr_scheduler.step()
                    optimizer.zero_grad()
                    
                    global_step += 1
                    
                    # Logging plus détaillé
                    resources = monitor_resources()
                    current_lr = lr_scheduler.get_last_lr()[0]
                    log.info(f"{progress} | Étape globale: {global_step} | Perte: {loss.item():.4f} | LR: {current_lr:.2e} | {resources}")
                else:
                    # Afficher la progression même pendant l'accumulation de gradient
                    if step % 5 == 0:  # Limiter la fréquence pour ne pas surcharger les logs
                        log.info(f"{progress} | Accumulation: {(step+1) % args.gradient_accumulation_steps}/{args.gradient_accumulation_steps} | Perte: {loss.item():.4f}")
                    
                    # Sauvegarde périodique
                    if global_step % args.save_steps == 0:
                        checkpoint_dir = os.path.join(output_dir, f"checkpoint-{global_step}")
                        os.makedirs(checkpoint_dir, exist_ok=True)
                        
                        # Sauvegarder le modèle
                        model.save_pretrained(checkpoint_dir)
                        tokenizer.save_pretrained(checkpoint_dir)
                        log.info(f"Checkpoint sauvegardé à l'étape {global_step}")
            
            # Accumuler la perte
            step_loss += loss.item() * args.gradient_accumulation_steps
            epoch_loss += loss.item() * args.gradient_accumulation_steps
        
        # Fin de l'époque avec statistiques détaillées
        avg_epoch_loss = epoch_loss / len(train_dataloader)
        log.info(f"{'='*20} Résumé de l'époque {epoch+1}/{args.epochs} {'='*20}")
        log.info(f"Perte moyenne: {avg_epoch_loss:.4f}")
        log.info(f"Étapes d'entraînement: {global_step}")
        log.info(f"Taux d'apprentissage actuel: {lr_scheduler.get_last_lr()[0]:.2e}")
        log.info(f"Temps écoulé pour cette époque: {time.time() - epoch_start_time:.2f} secondes")
        log.info(f"{'='*65}")
        
        # Évaluation sur le dataset de validation
        if val_dataloader:
            model.eval()
            val_loss = 0
            
            with torch.no_grad():
                for val_batch in val_dataloader:
                    val_batch = {k: v.to(model.device) for k, v in val_batch.items()}
                    val_outputs = model(**val_batch)
                    val_loss += val_outputs.loss.item()
            
            avg_val_loss = val_loss / len(val_dataloader)
            log.info(f"Validation | Perte: {avg_val_loss:.4f}")
            
            model.train()
    
    # Sauvegarder le modèle final
    log.info(f"Sauvegarde du modèle final dans {output_dir}")
    try:
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
    except Exception as e:
        log.error(f"Erreur lors de la sauvegarde du modèle: {e}")
        return
    
    # Test final du modèle
    test_model_responses(model, tokenizer)
    
    log.info("Fine-tuning terminé!")
    return True

if __name__ == "__main__":
    try:
        success = main()
        if success:
            sys.exit(0)
        else:
            sys.exit(1)
    except Exception as e:
        log.error(f"Erreur critique lors de l'exécution: {e}")
        import traceback
        log.error(traceback.format_exc())
        sys.exit(1)
