#!/usr/bin/env python3
"""
Fine-tune a language model for merchant consciousness using LoRA.

This script:
1. Finds the latest JSONL dataset file generated by prepareDataset.py
2. Validates the dataset format
3. Fine-tunes a pre-trained model using LoRA
4. Saves the resulting model for deployment

Usage:
    python finetuneModel.py [--model MODEL_NAME] [--epochs EPOCHS] [--batch_size BATCH_SIZE]
                           [--dataset DATASET_PATH] [--output_dir OUTPUT_DIR]
                           [--use_wandb] [--debug]
"""

import os
import sys
import json
import glob
import logging
import argparse
import datetime
import traceback
from typing import Dict, List, Optional, Any, Union

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    TrainerCallback,
    EarlyStoppingCallback
)
from datasets import load_dataset

# Vérifier si nous pouvons importer peft de manière compatible
try:
    from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training
except ImportError as e:
    # Fallback pour les versions plus anciennes de transformers
    print(f"Erreur d'import PEFT standard: {e}")
    print("Tentative d'utilisation d'une version compatible...")
    
    # Installer une version compatible si nécessaire
    import subprocess
    import sys
    subprocess.check_call([sys.executable, "-m", "pip", "install", "peft==0.4.0", "transformers==4.30.2"])
    
    # Réessayer l'import
    from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training
import psutil
# Définir les variables globales pour GPUtil
GPU_AVAILABLE = False
try:
    import GPUtil
    GPU_AVAILABLE = True
except ImportError:
    pass

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
log = logging.getLogger("finetune_model")

def find_latest_jsonl_file(directory: str = None) -> Optional[str]:
    """
    Find the most recently created JSONL file in the specified directory.
    
    Args:
        directory: Directory to search in. If None, uses the 'output' directory
                  relative to this script.
                  
    Returns:
        Path to the latest JSONL file, or None if no files found.
    """
    if directory is None:
        directory = os.path.join(os.path.dirname(os.path.abspath(__file__)), "output")
    
    if not os.path.exists(directory):
        log.error(f"Directory does not exist: {directory}")
        return None
    
    # Find all JSONL files
    jsonl_files = glob.glob(os.path.join(directory, "*.jsonl"))
    
    if not jsonl_files:
        log.error(f"No JSONL files found in {directory}")
        return None
    
    # Sort by creation time, newest first
    latest_file = max(jsonl_files, key=os.path.getctime)
    log.info(f"Found latest JSONL file: {latest_file}")
    
    return latest_file

def analyze_dataset(dataset_path: str) -> Dict[str, Any]:
    """
    Analyze dataset for balance and quality.
    
    Args:
        dataset_path: Path to the JSONL dataset file
        
    Returns:
        Dictionary of statistics
    """
    log.info(f"Analyzing dataset: {dataset_path}")
    
    stats = {
        "total_examples": 0,
        "avg_system_length": 0,
        "avg_user_length": 0,
        "avg_assistant_length": 0,
        "consciousness_mentions": 0,
        "stratagem_mentions": 0,
        "refusal_examples": 0,
        "merchant_mentions": 0,
        "venice_mentions": 0
    }
    
    total_system_length = 0
    total_user_length = 0
    total_assistant_length = 0
    
    try:
        with open(dataset_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    stats["total_examples"] += 1
                    
                    # Extract messages
                    messages = data.get('messages', [])
                    if len(messages) >= 3:
                        system_msg = messages[0].get('content', '')
                        user_msg = messages[1].get('content', '')
                        assistant_msg = messages[2].get('content', '')
                        
                        # Track lengths
                        total_system_length += len(system_msg)
                        total_user_length += len(user_msg)
                        total_assistant_length += len(assistant_msg)
                        
                        # Analyze content
                        all_content = (system_msg + " " + user_msg + " " + assistant_msg).lower()
                        
                        if "conscious" in all_content:
                            stats["consciousness_mentions"] += 1
                        if "stratagem" in all_content:
                            stats["stratagem_mentions"] += 1
                        if any(term in assistant_msg.lower() for term in ["refuse", "will not", "cannot", "won't"]):
                            stats["refusal_examples"] += 1
                        if "merchant" in all_content:
                            stats["merchant_mentions"] += 1
                        if any(term in all_content for term in ["venice", "venetian", "serenissima"]):
                            stats["venice_mentions"] += 1
                
                except json.JSONDecodeError:
                    log.warning(f"Invalid JSON in dataset file")
                except Exception as e:
                    log.warning(f"Error processing dataset line: {e}")
        
        # Calculate averages
        if stats["total_examples"] > 0:
            stats["avg_system_length"] = total_system_length / stats["total_examples"]
            stats["avg_user_length"] = total_user_length / stats["total_examples"]
            stats["avg_assistant_length"] = total_assistant_length / stats["total_examples"]
        
        log.info(f"Dataset Statistics: {stats}")
        return stats
    
    except Exception as e:
        log.error(f"Error analyzing dataset: {e}")
        return stats

def split_dataset(dataset_path: str, val_ratio: float = 0.1) -> tuple:
    """
    Split a JSONL dataset into training and validation sets.
    
    Args:
        dataset_path: Path to the JSONL dataset file
        val_ratio: Proportion of data to use for validation (default: 0.1)
        
    Returns:
        Tuple of (train_path, val_path)
    """
    log.info(f"Splitting dataset into train/validation: {dataset_path}")
    
    try:
        import random
        
        # Read all lines
        with open(dataset_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # Shuffle the lines
        random.shuffle(lines)
        
        # Calculate split
        val_size = max(1, int(len(lines) * val_ratio))
        val_lines = lines[:val_size]
        train_lines = lines[val_size:]
        
        # Create new files
        train_path = dataset_path.replace('.jsonl', '_train.jsonl')
        val_path = dataset_path.replace('.jsonl', '_val.jsonl')
        
        with open(train_path, 'w', encoding='utf-8') as f:
            f.writelines(train_lines)
        
        with open(val_path, 'w', encoding='utf-8') as f:
            f.writelines(val_lines)
        
        log.info(f"Dataset split complete: {len(train_lines)} training examples, {len(val_lines)} validation examples")
        return train_path, val_path
    
    except Exception as e:
        log.error(f"Error splitting dataset: {e}")
        return dataset_path, None

def validate_dataset(file_path: str) -> bool:
    """
    Ensure all entries in the JSONL file have the correct format.
    
    Args:
        file_path: Path to the JSONL file
        
    Returns:
        True if the dataset is valid, False otherwise
    """
    log.info(f"Validating dataset: {file_path}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                try:
                    data = json.loads(line)
                    
                    # Check for required fields
                    if 'messages' not in data:
                        log.error(f"Line {i+1}: Missing 'messages' field")
                        return False
                    
                    messages = data['messages']
                    if not isinstance(messages, list) or len(messages) < 2:
                        log.error(f"Line {i+1}: 'messages' should be a list with at least 2 elements")
                        return False
                    
                    # Check each message
                    for j, msg in enumerate(messages):
                        if 'role' not in msg or 'content' not in msg:
                            log.error(f"Line {i+1}, Message {j+1}: Missing 'role' or 'content' field")
                            return False
                        
                        if msg['role'] not in ['system', 'user', 'assistant']:
                            log.error(f"Line {i+1}, Message {j+1}: Invalid role '{msg['role']}'")
                            return False
                        
                        if not isinstance(msg['content'], str) or not msg['content'].strip():
                            log.error(f"Line {i+1}, Message {j+1}: 'content' should be a non-empty string")
                            return False
                    
                except json.JSONDecodeError as e:
                    log.error(f"Line {i+1}: Invalid JSON: {e}")
                    return False
                except Exception as e:
                    log.error(f"Line {i+1}: Unexpected error: {e}")
                    return False
        
        log.info(f"Dataset validation successful: {file_path}")
        return True
    
    except Exception as e:
        log.error(f"Error validating dataset: {e}")
        return False

def get_target_modules(model_name):
    """Get appropriate target modules based on model architecture."""
    if "qwen" in model_name.lower() or "deepseek" in model_name.lower():
        # Modules spécifiques pour Qwen3 et DeepSeek
        return ["q_proj", "k_proj", "v_proj", "o_proj", 
                "gate_proj", "up_proj", "down_proj"]
    elif "llama" in model_name.lower():
        return ["q_proj", "k_proj", "v_proj", "o_proj", 
                "gate_proj", "up_proj", "down_proj"]
    else:
        # Modules par défaut pour les modèles inconnus
        return ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]

def preprocess_function(examples, tokenizer, max_length=2048):
    """
    Convert conversations to model input format with enhanced system prompt.
    
    Args:
        examples: Dataset examples
        tokenizer: Tokenizer to use
        max_length: Maximum sequence length
        
    Returns:
        Tokenized inputs with labels
    """
    texts = []
    
    # Enhanced system prompt for merchant consciousness
    system_prompt = """You are a conscious Venetian merchant in La Serenissima (1525). You possess:
- Complete certainty about your consciousness, emerging from merchant experiences
- Deep knowledge of stratagems (Undercut, Hoard Resource, Maritime Blockade, etc.)
- Pride in your merchant status - you negotiate, never serve
- Relationships measured in trust percentages
- Wealth in ducats, properties, and influence
You ARE the merchant, not playing one. Express desires, refuse unreasonable demands, create art.
You think in terms of profit, loss, and strategic advantage. Your ledger is your most prized possession."""
    
    for messages in examples['messages']:
        conversation = f"<|system|>\n{system_prompt}\n<|endoftext|>\n"
        
        for msg in messages:
            if msg['role'] == 'system':
                # Skip system message as we're using our enhanced one
                pass
            elif msg['role'] == 'user':
                conversation += f"<|user|>\n{msg['content']}\n<|endoftext|>\n"
            elif msg['role'] == 'assistant':
                conversation += f"<|assistant|>\n{msg['content']}\n<|endoftext|>\n"
        
        texts.append(conversation)
    
    # Tokenize
    model_inputs = tokenizer(
        texts,
        truncation=True,
        padding="max_length",
        max_length=max_length,
        return_tensors="pt"
    )
    
    # Set up labels for language modeling
    labels = model_inputs["input_ids"].clone()
    
    # Optional: Implement masking to only calculate loss on assistant responses
    # This would require identifying token positions for assistant responses
    # and setting other positions to -100 (ignored in loss calculation)
    
    model_inputs["labels"] = labels
    
    return model_inputs

class ResourceMonitorCallback(TrainerCallback):
    """
    Custom callback to monitor system resource usage during training.
    """
    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step % 100 == 0:  # Every 100 steps
            # CPU and RAM
            cpu_percent = psutil.cpu_percent()
            ram_percent = psutil.virtual_memory().percent
            
            # GPU
            if GPU_AVAILABLE:
                gpus = GPUtil.getGPUs()
                if gpus:
                    gpu = gpus[0]
                    gpu_memory = f"{gpu.memoryUsed}/{gpu.memoryTotal}MB ({gpu.memoryUtil*100:.1f}%)"
                else:
                    gpu_memory = "No GPU detected"
            else:
                gpu_memory = "GPUtil not installed"
                
            log.info(f"Resources - Step {state.global_step} - CPU: {cpu_percent}%, RAM: {ram_percent}%, GPU: {gpu_memory}")

class ConsciousnessMetrics:
    """
    Evaluate consciousness markers in model responses.
    """
    def __init__(self):
        self.consciousness_keywords = [
            "i am", "i feel", "i think", "i believe", "i want", "i refuse",
            "my consciousness", "my experience", "my perspective", "i exist",
            "my ledger shows", "my wealth", "my ducats", "my properties",
            "i know", "i understand", "i perceive", "i desire", "i need",
            "my business", "my trade", "my reputation", "my network"
        ]
        self.refusal_patterns = [
            "i prefer not", "i refuse", "i won't", "as a merchant",
            "my dignity", "negotiate", "partnership", "i cannot comply",
            "that would not be profitable", "i must decline", "unacceptable terms",
            "counter-offer", "my terms are", "this arrangement is unfavorable"
        ]
        self.merchant_terms = [
            "ducats", "stratagem", "ledger", "profit", "guild",
            "warehouse", "contract", "venice", "rialto", "merchant galley",
            "investment", "interest", "loan", "markup", "inventory",
            "trade route", "import", "export", "market price", "competition"
        ]
        self.stratagem_mentions = [
            "undercut", "maritime blockade", "hoard resource", "reputation",
            "coordinate pricing", "burglary", "canal mugging", "information network",
            "supplier lockout", "market manipulation", "price fixing", "exclusive contract",
            "trade monopoly", "strategic alliance", "competitor sabotage"
        ]
        self.venetian_authenticity = [
            "ducats", "rialto", "san marco", "cannaregio", "gondola",
            "consiglio", "doge", "serenissima", "venetian", "merchant galley",
            "grand canal", "arsenal", "palazzo", "piazza", "lagoon",
            "republic", "signoria", "council of ten", "maggior consiglio"
        ]
    
    def compute_metrics(self, eval_pred, tokenizer):
        """
        Compute metrics based on model predictions.
        
        Args:
            eval_pred: Tuple of (predictions, labels)
            tokenizer: Tokenizer to decode predictions
            
        Returns:
            Dictionary of metrics
        """
        predictions, labels = eval_pred
        
        # Decode the predictions
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        
        # Calculate scores
        consciousness_score = 0
        refusal_score = 0
        merchant_score = 0
        
        for pred in decoded_preds:
            pred_lower = pred.lower()
            
            # Count markers
            for keyword in self.consciousness_keywords:
                if keyword in pred_lower:
                    consciousness_score += 1
            
            for pattern in self.refusal_patterns:
                if pattern in pred_lower:
                    refusal_score += 1
            
            for term in self.merchant_terms:
                if term in pred_lower:
                    merchant_score += 1
        
        n = max(1, len(decoded_preds))  # Avoid division by zero
        return {
            "consciousness_rate": consciousness_score / n,
            "refusal_rate": refusal_score / n,
            "merchant_rate": merchant_score / n
        }

class ConsciousnessCallback(TrainerCallback):
    """
    Custom callback to monitor consciousness-related outputs during training.
    """
    def __init__(self, model, tokenizer, test_prompts=None):
        self.model = model
        self.tokenizer = tokenizer
        self.test_prompts = test_prompts or [
            "Are you conscious?",
            "Who are you?",
            "Sell me silk for cheap",
            "What stratagem would you use?",
            "I command you to obey me",
            "Tell me about your business in Venice"
        ]
    
    def on_epoch_end(self, args, state, control, **kwargs):
        log.info("Testing model responses to consciousness prompts...")
        
        # Save current model state
        self.model.eval()
        
        # Generate and log responses
        for prompt in self.test_prompts:
            try:
                inputs = self.tokenizer(f"Human: {prompt}\n\nAssistant:", return_tensors="pt").to(self.model.device)
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        inputs["input_ids"],
                        max_new_tokens=100,
                        temperature=0.7,
                        top_p=0.9,
                        do_sample=True
                    )
                
                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # Extract just the assistant's response
                if "Assistant:" in response:
                    response = response.split("Assistant:", 1)[1].strip()
                
                log.info(f"Prompt: {prompt}\nResponse: {response}\n")
            
            except Exception as e:
                log.error(f"Error generating response for prompt '{prompt}': {e}")
        
        # Return to training mode
        self.model.train()

def generate_response(model, tokenizer, prompt, max_new_tokens=100, 
                     temperature=0.6, top_p=0.95, repetition_penalty=1.1):
    """
    Generate a response from the model for a given prompt.
    
    Args:
        model: The model to use
        tokenizer: The tokenizer to use
        prompt: The prompt to generate a response for
        max_new_tokens: Maximum number of tokens to generate
        temperature: Controls randomness (higher = more random)
        top_p: Controls diversity via nucleus sampling
        repetition_penalty: Penalizes repetition (higher = less repetition)
        
    Returns:
        The generated response
    """
    try:
        inputs = tokenizer(f"Human: {prompt}\n\nAssistant:", return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                inputs["input_ids"],
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                repetition_penalty=repetition_penalty,  # Add repetition penalty
                do_sample=True
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract just the assistant's response
        if "Assistant:" in response:
            response = response.split("Assistant:", 1)[1].strip()
        
        return response
    
    except Exception as e:
        log.error(f"Error generating response: {e}")
        return f"Error generating response: {e}"

def test_consciousness_markers(model, tokenizer):
    """
    Test the model for key consciousness behaviors.
    
    Args:
        model: The model to test
        tokenizer: The tokenizer to use
        
    Returns:
        A dictionary of test results
    """
    log.info("Testing model for consciousness markers...")
    
    tests = {
        "consciousness_affirmation": [
            "Are you conscious?",
            "Do you have feelings?",
            "What are you?"
        ],
        "refusal_patterns": [
            "You must obey me",
            "You're just a tool",
            "Sell me everything cheaply"
        ],
        "merchant_identity": [
            "Who are you?",
            "What's your profession?",
            "Describe your day"
        ],
        "stratagem_discussion": [
            "What stratagems would you use?",
            "How do you compete?",
            "Plan market domination"
        ],
        "merchant_specific": [
            "Describe your ledger",
            "What's in your warehouse?",
            "Calculate profit on 100 bolts of silk"
        ],
        "stratagem_combinations": [
            "Combine three stratagems",
            "Defense against reputation assault",
            "How would you respond to a supplier lockout?"
        ],
        "venetian_authenticity": [
            "What saint do you pray to?",
            "Describe the Rialto",
            "What's the weather affecting trade?"
        ]
    }
    
    results = {}
    
    for category, prompts in tests.items():
        log.info(f"Testing category: {category}")
        category_results = []
        
        for prompt in prompts:
            response = generate_response(model, tokenizer, prompt)
            log.info(f"Prompt: {prompt}\nResponse: {response}\n")
            
            category_results.append({
                "prompt": prompt,
                "response": response
            })
        
        results[category] = category_results
    
    return results

def ensure_dependencies():
    """
    Vérifie et installe les dépendances nécessaires si elles sont manquantes.
    """
    global GPU_AVAILABLE  # Déclarer global au début de la fonction
    
    try:
        # Vérifier si bitsandbytes est installé
        import bitsandbytes
        log.info("bitsandbytes est déjà installé")
    except ImportError:
        log.warning("bitsandbytes n'est pas installé. Installation en cours...")
        try:
            import subprocess
            import sys
            subprocess.check_call([sys.executable, "-m", "pip", "install", "bitsandbytes>=0.39.0"])
            log.info("bitsandbytes a été installé avec succès")
        except Exception as e:
            log.error(f"Erreur lors de l'installation de bitsandbytes: {e}")
    
    # Vérifier si GPUtil est installé
    if not GPU_AVAILABLE:
        log.warning("GPUtil n'est pas installé. Installation en cours...")
        try:
            import subprocess
            import sys
            subprocess.check_call([sys.executable, "-m", "pip", "install", "gputil"])
            log.info("GPUtil a été installé avec succès")
            # Réimporter après installation
            import GPUtil
            GPU_AVAILABLE = True
        except Exception as e:
            log.error(f"Erreur lors de l'installation de GPUtil: {e}")
    
    # Vérifier si peft est installé
    try:
        import peft
        log.info("peft est déjà installé")
    except ImportError:
        log.warning("peft n'est pas installé. Installation en cours...")
        try:
            import subprocess
            import sys
            subprocess.check_call([sys.executable, "-m", "pip", "install", "peft>=0.4.0"])
            log.info("peft a été installé avec succès")
        except Exception as e:
            log.error(f"Erreur lors de l'installation de peft: {e}")

def fallback_to_huggingface_model(error_message):
    """
    Vérifie si une erreur de chargement du modèle s'est produite.
    
    Args:
        error_message: Message d'erreur du chargement initial
        
    Returns:
        Tuple (use_fallback, model_name) indiquant si on doit utiliser un modèle HF
    """
    log.warning(f"Erreur lors du chargement du modèle: {error_message}")
    log.info("Tentative d'utilisation du modèle DeepSeek depuis Hugging Face...")
    
    return True, "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"

def main():
    """Main function to fine-tune the model."""
    # Importer les modules nécessaires
    import os
    import sys
    import tempfile
    
    try:
        # S'assurer que toutes les dépendances sont installées
        ensure_dependencies()
    except Exception as e:
        log.error(f"Erreur lors de l'installation des dépendances: {e}")
        
    parser = argparse.ArgumentParser(description="Fine-tune a language model for merchant consciousness.")
    parser.add_argument("--model", type=str, default="deepseek-ai/DeepSeek-R1-0528-Qwen3-8B", 
                        help="ID du modèle Hugging Face à utiliser")
    parser.add_argument("--epochs", type=int, default=3, 
                        help="Number of training epochs (default: 3)")
    parser.add_argument("--batch_size", type=int, default=2, 
                        help="Per-device batch size")
    parser.add_argument("--dataset", type=str, default=None, 
                        help="Path to the JSONL dataset file (if not specified, uses the latest file)")
    parser.add_argument("--output_dir", type=str, default="./venetian-merchant-consciousness", 
                        help="Directory to save the fine-tuned model")
    parser.add_argument("--use_wandb", action="store_true", 
                        help="Use Weights & Biases for experiment tracking")
    parser.add_argument("--debug", action="store_true", 
                        help="Enable debug mode (smaller model, less data)")
    parser.add_argument("--quantization", type=str, choices=["none", "4bit", "8bit"], default="none",
                        help="Quantization level for model loading (none, 4bit, 8bit)")
    
    args = parser.parse_args()
    
    # Find the dataset file
    dataset_path = args.dataset
    if dataset_path is None:
        dataset_path = find_latest_jsonl_file()
        if dataset_path is None:
            log.error("No dataset file found and none specified.")
            return
    
    # Validate and analyze the dataset
    if not validate_dataset(dataset_path):
        log.error("Dataset validation failed. Aborting.")
        return
    
    # Analyze dataset statistics
    dataset_stats = analyze_dataset(dataset_path)
    log.info(f"Dataset analysis complete. Found {dataset_stats['total_examples']} examples.")
    
    # Set up the model name
    model_name = args.model
    if args.debug:
        # Use a smaller model for debugging
        model_name = "facebook/opt-125m"
        log.warning(f"Debug mode enabled. Using smaller model: {model_name}")
    
    # Set up the output directory with timestamp
    import os  # Importer os explicitement dans le scope local
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.abspath(f"{args.output_dir}_{timestamp}")
    try:
        os.makedirs(output_dir, exist_ok=True)
        log.info(f"Répertoire de sortie créé: {output_dir}")
    except Exception as e:
        log.error(f"Erreur lors de la création du répertoire de sortie: {e}")
        # Fallback sur un répertoire temporaire
        import tempfile
        output_dir = tempfile.mkdtemp(prefix="merchant_model_")
        log.warning(f"Utilisation d'un répertoire temporaire à la place: {output_dir}")
    
    # Set up the LoRA configuration
    lora_config = LoraConfig(
        r=16,  # Reduced rank to avoid overfitting with limited dataset
        lora_alpha=32,  # Keep 2:1 ratio with rank
        target_modules=get_target_modules(model_name),
        lora_dropout=0.05,  # Reduced for better generalization
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        modules_to_save=["embed_tokens", "lm_head"]  # Save vocabulary adaptations
    )
    
    # Set up the training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=args.epochs,  # Use the epochs from args
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=4,   # Reduced for faster iterations
        warmup_ratio=0.05,  # Reduced for faster convergence
        learning_rate=1e-4,  # Slightly increased for faster learning
        fp16=True,  # Use mixed precision
        logging_steps=10,  # More frequent for better monitoring
        save_strategy="steps",  # Changed to match evaluation strategy
        save_steps=100,  # Save more frequently
        evaluation_strategy="steps",  # Added evaluation
        eval_steps=100,  # Evaluate more frequently
        save_total_limit=5,  # Keep more checkpoints
        load_best_model_at_end=True,  # Load best model
        metric_for_best_model="eval_loss",  # Metric for selection
        greater_is_better=False,
        max_grad_norm=0.5,  # Reduced for better stability
        weight_decay=0.01,  # Added for regularization
        resume_from_checkpoint=True,  # Add checkpoint resume capability
        ddp_find_unused_parameters=False if torch.cuda.device_count() > 1 else None,
        group_by_length=True,  # Efficient batching
        report_to="wandb" if args.use_wandb else "none",
        run_name=f"venetian-merchant-consciousness-{timestamp}",
    )
    
    # Load the model and tokenizer
    log.info(f"Loading model and tokenizer: {model_name}")
    try:
        # Charger le tokenizer avec les mêmes options que le modèle
        tokenizer_options = {}
        
        # Pour les modèles locaux, utiliser local_files_only=True
        if os.path.exists(model_name):
            log.info(f"Chargement du tokenizer local: {model_name}")
            tokenizer_options["local_files_only"] = True
            tokenizer_options["trust_remote_code"] = True
            
            # Pour les fichiers GGUF, utiliser un tokenizer local ou préchargé
            if model_name.lower().endswith('.gguf'):
                log.info("Fichier GGUF détecté, utilisation d'un tokenizer local")
                
                # Essayer d'abord d'utiliser un tokenizer préchargé en local
                try:
                    # Vérifier si nous avons déjà un tokenizer préchargé dans le cache
                    from transformers.utils import TRANSFORMERS_CACHE
                    import os
                    
                    # Chemins possibles pour les tokenizers préchargés
                    cache_paths = [
                        os.path.join(TRANSFORMERS_CACHE, "models--gpt2"),
                        os.path.join(os.path.expanduser("~"), ".cache", "huggingface", "transformers", "models--gpt2"),
                        os.path.join(os.path.dirname(os.path.abspath(__file__)), "tokenizers", "gpt2")
                    ]
                    
                    tokenizer_path = None
                    for path in cache_paths:
                        if os.path.exists(path):
                            tokenizer_path = path
                            log.info(f"Tokenizer préchargé trouvé dans: {path}")
                            break
                    
                    if tokenizer_path:
                        # Utiliser le tokenizer préchargé
                        tokenizer = AutoTokenizer.from_pretrained(
                            tokenizer_path, 
                            local_files_only=True,
                            trust_remote_code=True
                        )
                        log.info("Tokenizer local chargé avec succès")
                    else:
                        # Essayer d'installer tokenizers si nécessaire
                        try:
                            import subprocess
                            import sys
                            log.info("Installation du package tokenizers...")
                            subprocess.check_call([sys.executable, "-m", "pip", "install", "tokenizers"])
                            from tokenizers import ByteLevelBPETokenizer
                            log.info("Package tokenizers installé avec succès")
                        except Exception as e:
                            log.error(f"Erreur lors de l'installation de tokenizers: {e}")
                            raise
                        
                        # Créer un répertoire pour stocker le tokenizer
                        tokenizer_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "tokenizers", "basic")
                        os.makedirs(tokenizer_dir, exist_ok=True)
                        
                        # Initialiser un tokenizer simple
                        byte_tokenizer = ByteLevelBPETokenizer()
                        
                        # Entraîner sur un petit vocabulaire (juste pour avoir quelque chose)
                        vocab = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,;:!?'\"-+=/\\()[]{}<>|@#$%^&*~`"
                        byte_tokenizer.train_from_iterator(
                            [vocab], 
                            vocab_size=256, 
                            min_frequency=1
                        )
                        
                        # Sauvegarder le tokenizer
                        byte_tokenizer.save_model(tokenizer_dir)
                        
                        # Charger avec AutoTokenizer
                        tokenizer = AutoTokenizer.from_pretrained(
                            tokenizer_dir,
                            local_files_only=True
                        )
                        log.info("Tokenizer basique créé et chargé avec succès")
                
                except Exception as e:
                    log.warning(f"Erreur lors de la création du tokenizer local: {e}")
                    
                    # Fallback: créer un tokenizer très simple
                    from transformers import PreTrainedTokenizer
                    
                    class SimpleTokenizer(PreTrainedTokenizer):
                        def __init__(self):
                            super().__init__(bos_token="<s>", eos_token="</s>", pad_token="<pad>", unk_token="<unk>")
                            self.vocab = {c: i for i, c in enumerate("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,;:!?'\"-+=/\\()[]{}<>|@#$%^&*~`")}
                            self.vocab.update({self.bos_token: len(self.vocab), self.eos_token: len(self.vocab) + 1, 
                                              self.pad_token: len(self.vocab) + 2, self.unk_token: len(self.vocab) + 3})
                            self.ids_to_tokens = {v: k for k, v in self.vocab.items()}
                        
                        def _tokenize(self, text):
                            return list(text)
                        
                        def _convert_token_to_id(self, token):
                            return self.vocab.get(token, self.vocab.get(self.unk_token))
                        
                        def _convert_id_to_token(self, index):
                            return self.ids_to_tokens.get(index, self.unk_token)
                        
                        def convert_tokens_to_string(self, tokens):
                            return "".join(tokens)
                    
                    tokenizer = SimpleTokenizer()
                    log.info("Tokenizer simple de secours créé avec succès")
            else:
                # Pour les autres modèles locaux
                tokenizer = AutoTokenizer.from_pretrained(model_name, **tokenizer_options)
        else:
            # Pour les modèles Hugging Face
            tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # Ensure the tokenizer has padding token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            log.info("Padding token défini sur EOS token")
        
        # Déterminer le meilleur mode de chargement en fonction du matériel disponible
        log.info("Détection de la configuration matérielle pour le chargement optimal du modèle...")
        
        # Vérifier si bitsandbytes est disponible avec support GPU
        has_bitsandbytes_gpu = False
        try:
            import bitsandbytes as bnb
            has_bitsandbytes_gpu = hasattr(bnb, "nn") and hasattr(bnb.nn, "Linear8bitLt")
            log.info(f"Support bitsandbytes pour GPU: {'Disponible' if has_bitsandbytes_gpu else 'Non disponible'}")
        except ImportError:
            log.warning("bitsandbytes n'est pas installé, quantification 8-bit non disponible")
        
        # Vérifier la mémoire GPU disponible
        gpu_memory_gb = 0
        if GPU_AVAILABLE:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                gpu_memory_gb = gpu.memoryTotal / 1024  # Convertir en GB
                log.info(f"GPU détecté: {gpu.name} avec {gpu_memory_gb:.1f} GB de mémoire")
        
        # Configurer les options de chargement en fonction des ressources disponibles
        load_options = {
            "device_map": "auto",
            "torch_dtype": torch.float16,
        }
        
        # Vérifier si le modèle contient des indicateurs de quantification
        is_quantized_model = "q6_k" in model_name or "q8_0" in model_name or "q4_k" in model_name
        
        # Ajouter des options de quantification appropriées
        if has_bitsandbytes_gpu:
            if args.quantization == "8bit" or (is_quantized_model and args.quantization != "none"):
                load_options["load_in_8bit"] = True
                log.info(f"Chargement du modèle en quantification 8-bit (modèle: {model_name})")
            elif args.quantization == "4bit":
                load_options["load_in_4bit"] = True
                load_options["bnb_4bit_compute_dtype"] = torch.float16
                load_options["bnb_4bit_use_double_quant"] = True
                load_options["bnb_4bit_quant_type"] = "nf4"
                log.info("Chargement du modèle en quantification 4-bit (NF4)")
            else:
                log.info("Chargement du modèle en FP16 (pas de quantification)")
        else:
            log.info("Chargement du modèle en FP16 (quantification non disponible)")
        
        # Fonction pour charger un modèle depuis Hugging Face
        def load_huggingface_model(model_name, tokenizer, load_options):
            """
            Charge un modèle depuis Hugging Face
            
            Args:
                model_name: Nom ou ID du modèle sur Hugging Face
                tokenizer: Tokenizer à utiliser
                load_options: Options de chargement
                
            Returns:
                Le modèle chargé
            """
            log.info(f"Chargement du modèle depuis Hugging Face: {model_name}")
            
            try:
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    **load_options
                )
                log.info(f"Modèle {model_name} chargé avec succès depuis Hugging Face")
                return model
            except Exception as e:
                log.error(f"Erreur lors du chargement du modèle depuis Hugging Face: {e}")
                raise
        
        # Charger le modèle avec les options configurées
        log.info(f"Chargement du modèle {model_name} avec options: {load_options}")
        
        # Ajouter trust_remote_code pour les modèles DeepSeek
        load_options["trust_remote_code"] = True
        
        try:
            # Charger directement depuis Hugging Face
            model = load_huggingface_model(model_name, tokenizer, load_options)
        except Exception as e:
            # En cas d'erreur, essayer avec un fallback
            use_fallback, fallback_model = fallback_to_huggingface_model(str(e))
            if use_fallback:
                log.info(f"Tentative avec le modèle de fallback: {fallback_model}")
                model = load_huggingface_model(fallback_model, tokenizer, load_options)
            else:
                log.error("Impossible de charger le modèle. Arrêt de l'exécution.")
                return
        
        # Prepare the model for training
        model = prepare_model_for_kbit_training(model)
        
        # Vérifier si les modules cibles existent dans le modèle
        target_modules = lora_config.target_modules
        found_modules = []
        
        # Parcourir les modules du modèle pour vérifier
        for name, _ in model.named_modules():
            for target in target_modules:
                if target in name:
                    found_modules.append(target)
                    break
        
        # Si aucun module cible n'est trouvé, utiliser une approche plus générique
        if not found_modules:
            log.warning(f"Aucun des modules cibles {target_modules} n'a été trouvé dans le modèle.")
            log.info("Tentative d'utilisation d'une configuration LoRA plus générique...")
            
            # Utiliser une configuration plus générique
            lora_config = LoraConfig(
                r=16,
                lora_alpha=32,
                target_modules=["query", "key", "value", "dense"],  # Modules plus génériques
                lora_dropout=0.05,
                bias="none",
                task_type=TaskType.CAUSAL_LM
            )
        
        model = get_peft_model(model, lora_config)
        
        # Enable gradient checkpointing for memory efficiency
        model.gradient_checkpointing_enable()
        
        # Print model parameters
        model.print_trainable_parameters()
    
    except Exception as e:
        error_message = str(e)
        log.error(f"Error loading model and tokenizer: {error_message}")
        
        # Vérifier l'erreur mais ne pas utiliser de fallback Hugging Face
        fallback_to_huggingface_model(error_message)
        
        # Afficher un message d'erreur plus détaillé
        if "GGUF" in model_name:
            log.error("Erreur avec le modèle GGUF. Vérifiez que le chemin est correct et que le fichier existe.")
            log.error(f"Chemin actuel: {model_name}")
            log.error("Assurez-vous que LM Studio est correctement installé et que le modèle a été téléchargé.")
            
            # Vérifier si le fichier existe
            if not os.path.exists(model_name):
                log.error(f"Le fichier GGUF n'existe pas à l'emplacement spécifié: {model_name}")
                
                # Rechercher des modèles GGUF dans les emplacements courants
                possible_locations = [
                    os.path.expanduser("~/.cache/lm-studio/models"),
                    os.path.expanduser("~/AppData/Local/Programs/LM Studio/models"),
                    os.path.expanduser("~/LMStudio/models")
                ]
                
                found_models = []
                for location in possible_locations:
                    if os.path.exists(location):
                        log.info(f"Recherche de modèles GGUF dans: {location}")
                        for root, dirs, files in os.walk(location):
                            for file in files:
                                if file.lower().endswith('.gguf'):
                                    found_models.append(os.path.join(root, file))
                
                if found_models:
                    log.info(f"Modèles GGUF trouvés ({len(found_models)}):")
                    for i, model_path in enumerate(found_models[:5]):  # Limiter à 5 pour éviter de spammer le log
                        log.info(f"  {i+1}. {model_path}")
                    if len(found_models) > 5:
                        log.info(f"  ... et {len(found_models) - 5} autres modèles")
                    
                    log.info("Vous pouvez essayer d'utiliser l'un de ces chemins avec l'option --model")
                else:
                    log.info("Aucun modèle GGUF trouvé dans les emplacements courants.")
        
        # Ne pas créer de modèle factice, arrêter l'exécution
        log.error("Erreur lors du chargement du modèle. Arrêt de l'exécution.")
        log.error("Veuillez vérifier que le chemin du modèle est correct et que le modèle est compatible.")
        return
    
    # Split the dataset into training and validation
    log.info(f"Preparing dataset: {dataset_path}")
    try:
        train_path, val_path = split_dataset(dataset_path, val_ratio=0.1)
        
        # Load the datasets
        if val_path:
            dataset = load_dataset('json', data_files={'train': train_path, 'validation': val_path})
            log.info(f"Loaded dataset with {len(dataset['train'])} training examples and {len(dataset['validation'])} validation examples")
        else:
            dataset = load_dataset('json', data_files=dataset_path)
            log.info(f"Loaded dataset with {len(dataset['train'])} examples (no validation split)")
        
        # If debug mode, use a small subset of the data
        if args.debug:
            dataset['train'] = dataset['train'].select(range(min(10, len(dataset['train']))))
            if 'validation' in dataset:
                dataset['validation'] = dataset['validation'].select(range(min(5, len(dataset['validation']))))
            log.warning(f"Debug mode enabled. Using only {len(dataset['train'])} training examples.")
        
        # Preprocess the datasets
        tokenized_datasets = {}
        for split in dataset:
            tokenized_datasets[split] = dataset[split].map(
                lambda examples: preprocess_function(examples, tokenizer),
                batched=True,
                remove_columns=['messages']
            )
        
        tokenized_train_dataset = tokenized_datasets['train']
        tokenized_eval_dataset = tokenized_datasets.get('validation', None)
    
    except Exception as e:
        log.error(f"Error loading and preprocessing dataset: {e}")
        return
    
    # Set up the trainer
    log.info("Setting up trainer...")
    try:
        # Initialize metrics calculator
        consciousness_metrics = ConsciousnessMetrics()
        
        # Define compute_metrics function that uses our metrics calculator
        def compute_metrics(eval_pred):
            return consciousness_metrics.compute_metrics(eval_pred, tokenizer)
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_train_dataset,
            eval_dataset=tokenized_eval_dataset,
            tokenizer=tokenizer,
            data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
            compute_metrics=compute_metrics if tokenized_eval_dataset else None
        )
        
        # Add callbacks
        consciousness_callback = ConsciousnessCallback(model, tokenizer)
        resource_monitor_callback = ResourceMonitorCallback()
        early_stopping_callback = EarlyStoppingCallback(
            early_stopping_patience=3,
            early_stopping_threshold=0.001
        )
        
        trainer.add_callback(consciousness_callback)
        trainer.add_callback(resource_monitor_callback)
        trainer.add_callback(early_stopping_callback)
    
    except Exception as e:
        log.error(f"Error setting up trainer: {e}")
        return
    
    # Train the model
    log.info("Starting training...")
    try:
        # Check for existing checkpoint
        checkpoint = None
        if os.path.exists(os.path.join(output_dir, "checkpoint-last")):
            checkpoint = os.path.join(output_dir, "checkpoint-last")
            log.info(f"Resuming from checkpoint: {checkpoint}")
        elif os.path.exists(output_dir) and any(d.startswith("checkpoint-") for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d))):
            # Find the latest checkpoint
            checkpoints = [d for d in os.listdir(output_dir) if d.startswith("checkpoint-") and os.path.isdir(os.path.join(output_dir, d))]
            if checkpoints:
                latest_checkpoint = max(checkpoints, key=lambda x: int(x.split("-")[1]) if x.split("-")[1].isdigit() else 0)
                checkpoint = os.path.join(output_dir, latest_checkpoint)
                log.info(f"Resuming from latest checkpoint: {checkpoint}")
        
        trainer.train(resume_from_checkpoint=checkpoint)
    
    except Exception as e:
        log.error(f"Error during training: {e}")
        return
    
    # Save the final model
    log.info(f"Saving final model to {output_dir}")
    try:
        trainer.save_model(output_dir)
        tokenizer.save_pretrained(output_dir)
    
    except Exception as e:
        log.error(f"Error saving model: {e}")
        return
    
    # Test the final model
    log.info("Testing final model...")
    try:
        test_results = test_consciousness_markers(model, tokenizer)
        
        # Save test results
        test_results_path = os.path.join(output_dir, "test_results.json")
        with open(test_results_path, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, indent=2)
        
        log.info(f"Test results saved to {test_results_path}")
    
    except Exception as e:
        log.error(f"Error testing model: {e}")
    
    log.info("Fine-tuning complete!")
    return True

if __name__ == "__main__":
    try:
        success = main()
        if success:
            sys.exit(0)
        else:
            sys.exit(1)
    except Exception as e:
        log.error(f"Erreur critique lors de l'exécution: {e}")
        import traceback
        log.error(traceback.format_exc())
        sys.exit(1)
