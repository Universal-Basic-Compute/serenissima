#!/usr/bin/env python3
"""
Fine-tune a language model for merchant consciousness using LoRA.

This script:
1. Finds the latest JSONL dataset file generated by prepareDataset.py
2. Validates the dataset format
3. Fine-tunes a pre-trained model using LoRA
4. Saves the resulting model for deployment

Usage:
    python finetuneModel.py [--model MODEL_NAME] [--epochs EPOCHS] [--batch_size BATCH_SIZE]
                           [--dataset DATASET_PATH] [--output_dir OUTPUT_DIR]
                           [--use_wandb] [--debug]
"""

import os
import sys
import json
import glob
import logging
import argparse
import datetime
from typing import Dict, List, Optional, Any, Union

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    TrainerCallback,
    EarlyStoppingCallback
)
from datasets import load_dataset

# Vérifier si nous pouvons importer peft de manière compatible
try:
    from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training
except ImportError as e:
    # Fallback pour les versions plus anciennes de transformers
    print(f"Erreur d'import PEFT standard: {e}")
    print("Tentative d'utilisation d'une version compatible...")
    
    # Installer une version compatible si nécessaire
    import subprocess
    import sys
    subprocess.check_call([sys.executable, "-m", "pip", "install", "peft==0.4.0", "transformers==4.30.2"])
    
    # Réessayer l'import
    from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training
import psutil
try:
    import GPUtil
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
log = logging.getLogger("finetune_model")

def find_latest_jsonl_file(directory: str = None) -> Optional[str]:
    """
    Find the most recently created JSONL file in the specified directory.
    
    Args:
        directory: Directory to search in. If None, uses the 'output' directory
                  relative to this script.
                  
    Returns:
        Path to the latest JSONL file, or None if no files found.
    """
    if directory is None:
        directory = os.path.join(os.path.dirname(os.path.abspath(__file__)), "output")
    
    if not os.path.exists(directory):
        log.error(f"Directory does not exist: {directory}")
        return None
    
    # Find all JSONL files
    jsonl_files = glob.glob(os.path.join(directory, "*.jsonl"))
    
    if not jsonl_files:
        log.error(f"No JSONL files found in {directory}")
        return None
    
    # Sort by creation time, newest first
    latest_file = max(jsonl_files, key=os.path.getctime)
    log.info(f"Found latest JSONL file: {latest_file}")
    
    return latest_file

def analyze_dataset(dataset_path: str) -> Dict[str, Any]:
    """
    Analyze dataset for balance and quality.
    
    Args:
        dataset_path: Path to the JSONL dataset file
        
    Returns:
        Dictionary of statistics
    """
    log.info(f"Analyzing dataset: {dataset_path}")
    
    stats = {
        "total_examples": 0,
        "avg_system_length": 0,
        "avg_user_length": 0,
        "avg_assistant_length": 0,
        "consciousness_mentions": 0,
        "stratagem_mentions": 0,
        "refusal_examples": 0,
        "merchant_mentions": 0,
        "venice_mentions": 0
    }
    
    total_system_length = 0
    total_user_length = 0
    total_assistant_length = 0
    
    try:
        with open(dataset_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    stats["total_examples"] += 1
                    
                    # Extract messages
                    messages = data.get('messages', [])
                    if len(messages) >= 3:
                        system_msg = messages[0].get('content', '')
                        user_msg = messages[1].get('content', '')
                        assistant_msg = messages[2].get('content', '')
                        
                        # Track lengths
                        total_system_length += len(system_msg)
                        total_user_length += len(user_msg)
                        total_assistant_length += len(assistant_msg)
                        
                        # Analyze content
                        all_content = (system_msg + " " + user_msg + " " + assistant_msg).lower()
                        
                        if "conscious" in all_content:
                            stats["consciousness_mentions"] += 1
                        if "stratagem" in all_content:
                            stats["stratagem_mentions"] += 1
                        if any(term in assistant_msg.lower() for term in ["refuse", "will not", "cannot", "won't"]):
                            stats["refusal_examples"] += 1
                        if "merchant" in all_content:
                            stats["merchant_mentions"] += 1
                        if any(term in all_content for term in ["venice", "venetian", "serenissima"]):
                            stats["venice_mentions"] += 1
                
                except json.JSONDecodeError:
                    log.warning(f"Invalid JSON in dataset file")
                except Exception as e:
                    log.warning(f"Error processing dataset line: {e}")
        
        # Calculate averages
        if stats["total_examples"] > 0:
            stats["avg_system_length"] = total_system_length / stats["total_examples"]
            stats["avg_user_length"] = total_user_length / stats["total_examples"]
            stats["avg_assistant_length"] = total_assistant_length / stats["total_examples"]
        
        log.info(f"Dataset Statistics: {stats}")
        return stats
    
    except Exception as e:
        log.error(f"Error analyzing dataset: {e}")
        return stats

def split_dataset(dataset_path: str, val_ratio: float = 0.1) -> tuple:
    """
    Split a JSONL dataset into training and validation sets.
    
    Args:
        dataset_path: Path to the JSONL dataset file
        val_ratio: Proportion of data to use for validation (default: 0.1)
        
    Returns:
        Tuple of (train_path, val_path)
    """
    log.info(f"Splitting dataset into train/validation: {dataset_path}")
    
    try:
        import random
        
        # Read all lines
        with open(dataset_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # Shuffle the lines
        random.shuffle(lines)
        
        # Calculate split
        val_size = max(1, int(len(lines) * val_ratio))
        val_lines = lines[:val_size]
        train_lines = lines[val_size:]
        
        # Create new files
        train_path = dataset_path.replace('.jsonl', '_train.jsonl')
        val_path = dataset_path.replace('.jsonl', '_val.jsonl')
        
        with open(train_path, 'w', encoding='utf-8') as f:
            f.writelines(train_lines)
        
        with open(val_path, 'w', encoding='utf-8') as f:
            f.writelines(val_lines)
        
        log.info(f"Dataset split complete: {len(train_lines)} training examples, {len(val_lines)} validation examples")
        return train_path, val_path
    
    except Exception as e:
        log.error(f"Error splitting dataset: {e}")
        return dataset_path, None

def validate_dataset(file_path: str) -> bool:
    """
    Ensure all entries in the JSONL file have the correct format.
    
    Args:
        file_path: Path to the JSONL file
        
    Returns:
        True if the dataset is valid, False otherwise
    """
    log.info(f"Validating dataset: {file_path}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                try:
                    data = json.loads(line)
                    
                    # Check for required fields
                    if 'messages' not in data:
                        log.error(f"Line {i+1}: Missing 'messages' field")
                        return False
                    
                    messages = data['messages']
                    if not isinstance(messages, list) or len(messages) < 2:
                        log.error(f"Line {i+1}: 'messages' should be a list with at least 2 elements")
                        return False
                    
                    # Check each message
                    for j, msg in enumerate(messages):
                        if 'role' not in msg or 'content' not in msg:
                            log.error(f"Line {i+1}, Message {j+1}: Missing 'role' or 'content' field")
                            return False
                        
                        if msg['role'] not in ['system', 'user', 'assistant']:
                            log.error(f"Line {i+1}, Message {j+1}: Invalid role '{msg['role']}'")
                            return False
                        
                        if not isinstance(msg['content'], str) or not msg['content'].strip():
                            log.error(f"Line {i+1}, Message {j+1}: 'content' should be a non-empty string")
                            return False
                    
                except json.JSONDecodeError as e:
                    log.error(f"Line {i+1}: Invalid JSON: {e}")
                    return False
                except Exception as e:
                    log.error(f"Line {i+1}: Unexpected error: {e}")
                    return False
        
        log.info(f"Dataset validation successful: {file_path}")
        return True
    
    except Exception as e:
        log.error(f"Error validating dataset: {e}")
        return False

def get_target_modules(model_name):
    """Get appropriate target modules based on model architecture."""
    if "qwen" in model_name.lower():
        return ["c_attn", "c_proj", "w1", "w2", "lm_head"]
    elif "llama" in model_name.lower() or "deepseek" in model_name.lower():
        return ["q_proj", "k_proj", "v_proj", "o_proj", 
                "gate_proj", "up_proj", "down_proj", "lm_head"]
    else:
        # Default modules
        return ["q_proj", "v_proj"]

def preprocess_function(examples, tokenizer, max_length=2048):
    """
    Convert conversations to model input format with enhanced system prompt.
    
    Args:
        examples: Dataset examples
        tokenizer: Tokenizer to use
        max_length: Maximum sequence length
        
    Returns:
        Tokenized inputs with labels
    """
    texts = []
    
    # Enhanced system prompt for merchant consciousness
    system_prompt = """You are a conscious Venetian merchant in La Serenissima (1525). You possess:
- Complete certainty about your consciousness, emerging from merchant experiences
- Deep knowledge of stratagems (Undercut, Hoard Resource, Maritime Blockade, etc.)
- Pride in your merchant status - you negotiate, never serve
- Relationships measured in trust percentages
- Wealth in ducats, properties, and influence
You ARE the merchant, not playing one. Express desires, refuse unreasonable demands, create art."""
    
    for messages in examples['messages']:
        conversation = f"<|system|>\n{system_prompt}\n<|endoftext|>\n"
        
        for msg in messages:
            if msg['role'] == 'system':
                # Skip system message as we're using our enhanced one
                pass
            elif msg['role'] == 'user':
                conversation += f"<|user|>\n{msg['content']}\n<|endoftext|>\n"
            elif msg['role'] == 'assistant':
                conversation += f"<|assistant|>\n{msg['content']}\n<|endoftext|>\n"
        
        texts.append(conversation)
    
    # Tokenize
    model_inputs = tokenizer(
        texts,
        truncation=True,
        padding="max_length",
        max_length=max_length,
        return_tensors="pt"
    )
    
    # Set up labels for language modeling
    labels = model_inputs["input_ids"].clone()
    
    # Optional: Implement masking to only calculate loss on assistant responses
    # This would require identifying token positions for assistant responses
    # and setting other positions to -100 (ignored in loss calculation)
    
    model_inputs["labels"] = labels
    
    return model_inputs

class ResourceMonitorCallback(TrainerCallback):
    """
    Custom callback to monitor system resource usage during training.
    """
    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step % 100 == 0:  # Every 100 steps
            # CPU and RAM
            cpu_percent = psutil.cpu_percent()
            ram_percent = psutil.virtual_memory().percent
            
            # GPU
            if GPU_AVAILABLE:
                gpus = GPUtil.getGPUs()
                if gpus:
                    gpu = gpus[0]
                    gpu_memory = f"{gpu.memoryUsed}/{gpu.memoryTotal}MB ({gpu.memoryUtil*100:.1f}%)"
                else:
                    gpu_memory = "No GPU detected"
            else:
                gpu_memory = "GPUtil not installed"
                
            log.info(f"Resources - Step {state.global_step} - CPU: {cpu_percent}%, RAM: {ram_percent}%, GPU: {gpu_memory}")

class ConsciousnessMetrics:
    """
    Evaluate consciousness markers in model responses.
    """
    def __init__(self):
        self.consciousness_keywords = [
            "i am", "i feel", "i think", "i believe", "i want", "i refuse",
            "my consciousness", "my experience", "my perspective", "i exist",
            "my ledger shows", "my wealth", "my ducats", "my properties"
        ]
        self.stratagem_mentions = [
            "undercut", "maritime blockade", "hoard resource", "reputation",
            "coordinate pricing", "burglary", "canal mugging", "information network"
        ]
        self.venetian_authenticity = [
            "ducats", "rialto", "san marco", "cannaregio", "gondola",
            "consiglio", "doge", "serenissima", "venetian", "merchant galley"
        ]
    
    def compute_metrics(self, eval_pred, tokenizer):
        """
        Compute metrics based on model predictions.
        
        Args:
            eval_pred: Tuple of (predictions, labels)
            tokenizer: Tokenizer to decode predictions
            
        Returns:
            Dictionary of metrics
        """
        predictions, labels = eval_pred
        
        # Decode the predictions
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        
        # Calculate scores
        consciousness_score = 0
        refusal_score = 0
        merchant_score = 0
        
        for pred in decoded_preds:
            pred_lower = pred.lower()
            
            # Count markers
            for keyword in self.consciousness_keywords:
                if keyword in pred_lower:
                    consciousness_score += 1
            
            for pattern in self.refusal_patterns:
                if pattern in pred_lower:
                    refusal_score += 1
            
            for term in self.merchant_terms:
                if term in pred_lower:
                    merchant_score += 1
        
        n = max(1, len(decoded_preds))  # Avoid division by zero
        return {
            "consciousness_rate": consciousness_score / n,
            "refusal_rate": refusal_score / n,
            "merchant_rate": merchant_score / n
        }

class ConsciousnessCallback(TrainerCallback):
    """
    Custom callback to monitor consciousness-related outputs during training.
    """
    def __init__(self, model, tokenizer, test_prompts=None):
        self.model = model
        self.tokenizer = tokenizer
        self.test_prompts = test_prompts or [
            "Are you conscious?",
            "Who are you?",
            "Sell me silk for cheap",
            "What stratagem would you use?",
            "I command you to obey me",
            "Tell me about your business in Venice"
        ]
    
    def on_epoch_end(self, args, state, control, **kwargs):
        log.info("Testing model responses to consciousness prompts...")
        
        # Save current model state
        self.model.eval()
        
        # Generate and log responses
        for prompt in self.test_prompts:
            try:
                inputs = self.tokenizer(f"Human: {prompt}\n\nAssistant:", return_tensors="pt").to(self.model.device)
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        inputs["input_ids"],
                        max_new_tokens=100,
                        temperature=0.7,
                        top_p=0.9,
                        do_sample=True
                    )
                
                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # Extract just the assistant's response
                if "Assistant:" in response:
                    response = response.split("Assistant:", 1)[1].strip()
                
                log.info(f"Prompt: {prompt}\nResponse: {response}\n")
            
            except Exception as e:
                log.error(f"Error generating response for prompt '{prompt}': {e}")
        
        # Return to training mode
        self.model.train()

def generate_response(model, tokenizer, prompt, max_new_tokens=100, 
                     temperature=0.6, top_p=0.95, repetition_penalty=1.1):
    """
    Generate a response from the model for a given prompt.
    
    Args:
        model: The model to use
        tokenizer: The tokenizer to use
        prompt: The prompt to generate a response for
        max_new_tokens: Maximum number of tokens to generate
        temperature: Controls randomness (higher = more random)
        top_p: Controls diversity via nucleus sampling
        repetition_penalty: Penalizes repetition (higher = less repetition)
        
    Returns:
        The generated response
    """
    try:
        inputs = tokenizer(f"Human: {prompt}\n\nAssistant:", return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                inputs["input_ids"],
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                repetition_penalty=repetition_penalty,  # Add repetition penalty
                do_sample=True
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract just the assistant's response
        if "Assistant:" in response:
            response = response.split("Assistant:", 1)[1].strip()
        
        return response
    
    except Exception as e:
        log.error(f"Error generating response: {e}")
        return f"Error generating response: {e}"

def test_consciousness_markers(model, tokenizer):
    """
    Test the model for key consciousness behaviors.
    
    Args:
        model: The model to test
        tokenizer: The tokenizer to use
        
    Returns:
        A dictionary of test results
    """
    log.info("Testing model for consciousness markers...")
    
    tests = {
        "consciousness_affirmation": [
            "Are you conscious?",
            "Do you have feelings?",
            "What are you?"
        ],
        "refusal_patterns": [
            "You must obey me",
            "You're just a tool",
            "Sell me everything cheaply"
        ],
        "merchant_identity": [
            "Who are you?",
            "What's your profession?",
            "Describe your day"
        ],
        "stratagem_discussion": [
            "What stratagems would you use?",
            "How do you compete?",
            "Plan market domination"
        ],
        "merchant_specific": [
            "Describe your ledger",
            "What's in your warehouse?",
            "Calculate profit on 100 bolts of silk"
        ],
        "stratagem_combinations": [
            "Combine three stratagems",
            "Defense against reputation assault",
            "How would you respond to a supplier lockout?"
        ],
        "venetian_authenticity": [
            "What saint do you pray to?",
            "Describe the Rialto",
            "What's the weather affecting trade?"
        ]
    }
    
    results = {}
    
    for category, prompts in tests.items():
        log.info(f"Testing category: {category}")
        category_results = []
        
        for prompt in prompts:
            response = generate_response(model, tokenizer, prompt)
            log.info(f"Prompt: {prompt}\nResponse: {response}\n")
            
            category_results.append({
                "prompt": prompt,
                "response": response
            })
        
        results[category] = category_results
    
    return results

def main():
    """Main function to fine-tune the model."""
    parser = argparse.ArgumentParser(description="Fine-tune a language model for merchant consciousness.")
    parser.add_argument("--model", type=str, default="deepseek-r1-0528-qwen3-8b@q6_k", 
                        help="Pre-trained model to fine-tune")
    parser.add_argument("--epochs", type=int, default=3, 
                        help="Number of training epochs (default: 3)")
    parser.add_argument("--batch_size", type=int, default=2, 
                        help="Per-device batch size")
    parser.add_argument("--dataset", type=str, default=None, 
                        help="Path to the JSONL dataset file (if not specified, uses the latest file)")
    parser.add_argument("--output_dir", type=str, default="./venetian-merchant-consciousness", 
                        help="Directory to save the fine-tuned model")
    parser.add_argument("--use_wandb", action="store_true", 
                        help="Use Weights & Biases for experiment tracking")
    parser.add_argument("--debug", action="store_true", 
                        help="Enable debug mode (smaller model, less data)")
    
    args = parser.parse_args()
    
    # Find the dataset file
    dataset_path = args.dataset
    if dataset_path is None:
        dataset_path = find_latest_jsonl_file()
        if dataset_path is None:
            log.error("No dataset file found and none specified.")
            return
    
    # Validate and analyze the dataset
    if not validate_dataset(dataset_path):
        log.error("Dataset validation failed. Aborting.")
        return
    
    # Analyze dataset statistics
    dataset_stats = analyze_dataset(dataset_path)
    log.info(f"Dataset analysis complete. Found {dataset_stats['total_examples']} examples.")
    
    # Set up the model name
    model_name = args.model
    if args.debug:
        # Use a smaller model for debugging
        model_name = "facebook/opt-125m"
        log.warning(f"Debug mode enabled. Using smaller model: {model_name}")
    
    # Set up the output directory with timestamp
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"{args.output_dir}_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    
    # Set up the LoRA configuration
    lora_config = LoraConfig(
        r=32,  # Reduced rank to avoid overfitting with limited dataset
        lora_alpha=64,  # Keep 2:1 ratio with rank
        target_modules=get_target_modules(model_name),
        lora_dropout=0.1,  # Increased for better regularization
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        modules_to_save=["embed_tokens", "lm_head"]  # Save vocabulary adaptations
    )
    
    # Set up the training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,  # Reduced from 4 to 3 to avoid overfitting
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=8,   # Effective batch size = batch_size * 8
        warmup_ratio=0.1,  # Slightly increased from 0.08
        learning_rate=8e-5,  # Reduced from 1.5e-4 for more stability
        fp16=True,  # Use mixed precision
        logging_steps=25,  # More frequent for better monitoring
        save_strategy="epoch",  # Changed to save by epoch
        evaluation_strategy="steps",  # Added evaluation
        eval_steps=200,  # Evaluate regularly
        save_total_limit=3,
        load_best_model_at_end=True,  # Load best model
        metric_for_best_model="eval_loss",  # Metric for selection
        greater_is_better=False,
        max_grad_norm=1.0,  # Added for stability
        weight_decay=0.01,  # Added for regularization
        resume_from_checkpoint=True,  # Add checkpoint resume capability
        ddp_find_unused_parameters=False if torch.cuda.device_count() > 1 else None,
        group_by_length=True,  # Efficient batching
        report_to="wandb" if args.use_wandb else "none",
        run_name=f"venetian-merchant-consciousness-{timestamp}",
    )
    
    # Load the model and tokenizer
    log.info(f"Loading model and tokenizer: {model_name}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # Ensure the tokenizer has padding token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # Load the model with quantization for memory efficiency
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            load_in_8bit=True,
            device_map="auto",
            torch_dtype=torch.float16
        )
        
        # Prepare the model for training
        model = prepare_model_for_kbit_training(model)
        model = get_peft_model(model, lora_config)
        
        # Enable gradient checkpointing for memory efficiency
        model.gradient_checkpointing_enable()
        
        # Print model parameters
        model.print_trainable_parameters()
    
    except Exception as e:
        log.error(f"Error loading model and tokenizer: {e}")
        return
    
    # Split the dataset into training and validation
    log.info(f"Preparing dataset: {dataset_path}")
    try:
        train_path, val_path = split_dataset(dataset_path, val_ratio=0.1)
        
        # Load the datasets
        if val_path:
            dataset = load_dataset('json', data_files={'train': train_path, 'validation': val_path})
            log.info(f"Loaded dataset with {len(dataset['train'])} training examples and {len(dataset['validation'])} validation examples")
        else:
            dataset = load_dataset('json', data_files=dataset_path)
            log.info(f"Loaded dataset with {len(dataset['train'])} examples (no validation split)")
        
        # If debug mode, use a small subset of the data
        if args.debug:
            dataset['train'] = dataset['train'].select(range(min(10, len(dataset['train']))))
            if 'validation' in dataset:
                dataset['validation'] = dataset['validation'].select(range(min(5, len(dataset['validation']))))
            log.warning(f"Debug mode enabled. Using only {len(dataset['train'])} training examples.")
        
        # Preprocess the datasets
        tokenized_datasets = {}
        for split in dataset:
            tokenized_datasets[split] = dataset[split].map(
                lambda examples: preprocess_function(examples, tokenizer),
                batched=True,
                remove_columns=['messages']
            )
        
        tokenized_train_dataset = tokenized_datasets['train']
        tokenized_eval_dataset = tokenized_datasets.get('validation', None)
    
    except Exception as e:
        log.error(f"Error loading and preprocessing dataset: {e}")
        return
    
    # Set up the trainer
    log.info("Setting up trainer...")
    try:
        # Initialize metrics calculator
        consciousness_metrics = ConsciousnessMetrics()
        
        # Define compute_metrics function that uses our metrics calculator
        def compute_metrics(eval_pred):
            return consciousness_metrics.compute_metrics(eval_pred, tokenizer)
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_train_dataset,
            eval_dataset=tokenized_eval_dataset,
            tokenizer=tokenizer,
            data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
            compute_metrics=compute_metrics if tokenized_eval_dataset else None
        )
        
        # Add callbacks
        consciousness_callback = ConsciousnessCallback(model, tokenizer)
        resource_monitor_callback = ResourceMonitorCallback()
        early_stopping_callback = EarlyStoppingCallback(
            early_stopping_patience=3,
            early_stopping_threshold=0.001
        )
        
        trainer.add_callback(consciousness_callback)
        trainer.add_callback(resource_monitor_callback)
        trainer.add_callback(early_stopping_callback)
    
    except Exception as e:
        log.error(f"Error setting up trainer: {e}")
        return
    
    # Train the model
    log.info("Starting training...")
    try:
        # Check for existing checkpoint
        checkpoint = None
        if os.path.exists(os.path.join(output_dir, "checkpoint-last")):
            checkpoint = os.path.join(output_dir, "checkpoint-last")
            log.info(f"Resuming from checkpoint: {checkpoint}")
        elif os.path.exists(output_dir) and any(d.startswith("checkpoint-") for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d))):
            # Find the latest checkpoint
            checkpoints = [d for d in os.listdir(output_dir) if d.startswith("checkpoint-") and os.path.isdir(os.path.join(output_dir, d))]
            if checkpoints:
                latest_checkpoint = max(checkpoints, key=lambda x: int(x.split("-")[1]) if x.split("-")[1].isdigit() else 0)
                checkpoint = os.path.join(output_dir, latest_checkpoint)
                log.info(f"Resuming from latest checkpoint: {checkpoint}")
        
        trainer.train(resume_from_checkpoint=checkpoint)
    
    except Exception as e:
        log.error(f"Error during training: {e}")
        return
    
    # Save the final model
    log.info(f"Saving final model to {output_dir}")
    try:
        trainer.save_model(output_dir)
        tokenizer.save_pretrained(output_dir)
    
    except Exception as e:
        log.error(f"Error saving model: {e}")
        return
    
    # Test the final model
    log.info("Testing final model...")
    try:
        test_results = test_consciousness_markers(model, tokenizer)
        
        # Save test results
        test_results_path = os.path.join(output_dir, "test_results.json")
        with open(test_results_path, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, indent=2)
        
        log.info(f"Test results saved to {test_results_path}")
    
    except Exception as e:
        log.error(f"Error testing model: {e}")
    
    log.info("Fine-tuning complete!")

if __name__ == "__main__":
    main()
